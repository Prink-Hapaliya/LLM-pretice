{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:/Users/prink/Documents/git_llm_pretice/rag/speech.txt'}, page_content='A noun is a word that names a person, place, concept, or object. Basically, anything that names a “thing” is a noun, whether you’re talking about a basketball court, San Francisco, Cleopatra, or self-preservation.\\n\\nNouns fall into two categories: common nouns and proper nouns. Common nouns are general names for things, like planet and game show. Proper nouns are names or titles for specific things, like Jupiter and Jeopardy!\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"C:/Users/prink/Documents/git_llm_pretice/rag/speech.txt\"\n",
    "\n",
    "# Replace 'utf-8' with the detected encoding if different\n",
    "loader = TextLoader(file_path, encoding='utf-8')  \n",
    "try:\n",
    "    text_documents = loader.load()\n",
    "    print(text_documents)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Define web_paths as a list containing the URL\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\"],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-title\", \"post-content\", \"post-header\")  # Ensure correct class names\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load the documents\n",
    "text_documents = loader.load()\n",
    "\n",
    "# Print the loaded documents\n",
    "print(text_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 0}, page_content='LLM documentation\\nRelease 0.16-1-gd654c95\\nSimon Willison\\nSep 12, 2024'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 1}, page_content=''),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 2}, page_content='CONTENTS\\n1 Quick start 3\\n2 Contents 5\\n2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.1 Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.2 Upgrading to the latest version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.3 A note about Homebrew and PyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.4 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.5 API key management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.6 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2.1 Executing a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2.2 Completion prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.3 Continuing a conversation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.4 Using with a shell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.2.5 System prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.2.6 Starting an interactive chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.2.7 Listing available models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3 OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.3.1 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.3.2 OpenAI language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.3.3 OpenAI embedding models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.3.4 Adding more OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.4 Other models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.4.1 Installing and using a local model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.4.2 OpenAI-compatible models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.5 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.5.1 Embedding with the CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.5.2 Using embeddings from Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.5.3 Writing plugins to add new embedding models . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.5.4 Embedding storage format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.6 Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.6.1 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.6.2 Plugin directory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.6.3 Plugin hooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.6.4 Writing a plugin to support a new model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6.5 Utility functions for plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n2.7 Model aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.7.1 Listing aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\n2.7.2 Adding a new alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\ni'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 3}, page_content='2.7.3 Removing an alias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n2.7.4 Viewing the aliases file . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n2.8 Python API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n2.8.1 Basic prompt execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n2.8.2 Streaming responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n2.8.3 Conversations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n2.8.4 Other functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n2.9 Prompt templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n2.9.1 Getting started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n2.9.2 Using a template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n2.9.3 Listing available templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n2.9.4 Templates as YAML files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n2.9.5 System templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n2.9.6 Additional template variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n2.9.7 Specifying default parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.9.8 Setting a default model for a template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.10 Logging to SQLite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.10.1 Viewing the logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n2.10.2 SQL schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n2.11 Related tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n2.11.1 strip-tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n2.11.2 ttok . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n2.11.3 Symbex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n2.12 CLI reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n2.12.1 llm –help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n2.13 Contributing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n2.13.1 Debugging tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n2.13.2 Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n2.13.3 Release process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n2.14 Changelog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n2.14.1 0.16 (2024-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n2.14.2 0.15 (2024-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n2.14.3 0.14 (2024-05-13) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n2.14.4 0.13.1 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n2.14.5 0.13 (2024-01-26) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n2.14.6 0.12 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n2.14.7 0.11.2 (2023-11-06) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n2.14.8 0.11.1 (2023-10-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n2.14.9 0.11 (2023-09-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n2.14.10 0.10 (2023-09-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n2.14.11 0.10a1 (2023-09-11) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n2.14.12 0.10a0 (2023-09-04) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n2.14.13 0.9 (2023-09-03) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n2.14.14 0.8.1 (2023-08-31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n2.14.15 0.8 (2023-08-20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n2.14.16 0.7.1 (2023-08-19) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n2.14.17 0.7 (2023-08-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n2.14.18 0.6.1 (2023-07-24) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\\n2.14.19 0.6 (2023-07-18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n2.14.20 0.5 (2023-07-12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n2.14.21 0.4.1 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\\n2.14.22 0.4 (2023-06-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\\n2.14.23 0.3 (2023-05-17) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\\n2.14.24 0.2 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\\nii'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 4}, page_content='2.14.25 0.1 (2023-04-01) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\\niii'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 5}, page_content='iv'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 6}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nA CLI utility and Python library for interacting with Large Language Models, both via remote APIs and models that\\ncan be installed and run on your own machine.\\nRun prompts from the command-line ,store the results in SQLite ,generate embeddings and more.\\nHere’s a YouTube video demo and accompanying detailed notes.\\nBackground on this project:\\n•llm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs\\n•The LLM CLI tool now supports self-hosted language models via plugins\\n•Accessing Llama 2 from the command-line with the llm-replicate plugin\\n•Run Llama 2 on your own Mac using LLM and Homebrew\\n•Catching up on the weird world of LLMs\\n•LLM now provides tools for working with embeddings\\n•Build an image search engine with llm-clip, chat with models with llm chat\\n•Many options for running Mistral models in your terminal using LLM\\nFor more check out the llm tag on my blog.\\nCONTENTS 1'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 7}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2 CONTENTS'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 8}, page_content='CHAPTER\\nONE\\nQUICK START\\nFirst, install LLM using pipor Homebrew or pipx:\\npip install llm\\nOr with Homebrew (see warning note ):\\nbrew install llm\\nOr with pipx:\\npipx install llm\\nIf you have an OpenAI API key key you can run this:\\n# Paste your OpenAI API key into this\\nllm keys set openai\\n# Run a prompt\\nllm \"Ten fun names for a pet pelican\"\\n# Run a system prompt against a file\\ncat myfile.py | llm -s \"Explain this code\"\\nOr you can install a plugin and use models that can run on your local device:\\n# Install the plugin\\nllm install llm-gpt4all\\n# Download and run a prompt against the Orca Mini 7B model\\nllm -m orca-mini-3b-gguf2-q4_0 \\'What is the capital of France? \\'\\nTo startan interactive chat with a model, use llm chat :\\nllm chat -m gpt-4o-mini\\nChatting with gpt-4o-mini\\nType \\'exit \\'or\\'quit \\'to exit\\nType \\'!multi \\'to enter multiple lines, then \\'!end \\'to finish\\n> Tell me a joke about a pelican\\nWhy don \\'t pelicans like to tip waiters?\\n(continues on next page)\\n3'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 9}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nBecause they always have a big bill!\\n>\\n4 Chapter 1. Quick start'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 10}, page_content='CHAPTER\\nTWO\\nCONTENTS\\n2.1 Setup\\n2.1.1 Installation\\nInstall this tool using pip:\\npip install llm\\nOr using pipx:\\npipx install llm\\nOr using Homebrew (see warning note ):\\nbrew install llm\\n2.1.2 Upgrading to the latest version\\nIf you installed using pip:\\npip install -U llm\\nForpipx:\\npipx upgrade llm\\nFor Homebrew:\\nbrew upgrade llm\\nIf the latest version is not yet available on Homebrew you can upgrade like this instead:\\nllm install -U llm\\n5'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 11}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.1.3 A note about Homebrew and PyTorch\\nTheversionofLLMpackagedforHomebrewcurrentlyusesPython3.12. ThePyTorchprojectdonotyethaveastable\\nrelease of PyTorch for that version of Python.\\nThis means that LLM plugins that depend on PyTorch such as llm-sentence-transformers may not install cleanly with\\nthe Homebrew version of LLM.\\nYou can workaround this by manually installing PyTorch before installing llm-sentence-transformers :\\nllm install llm-python\\nllm python -m pip install \\\\\\n--pre torch torchvision \\\\\\n--index-url https://download.pytorch.org/whl/nightly/cpu\\nllm install llm-sentence-transformers\\nThis should produce a working installation of that plugin.\\n2.1.4 Installing plugins\\nPluginscan be used to add support for other language models, including models that can run on your own device.\\nFor example, the llm-gpt4all plugin adds support for 17 new models that can be installed on your own machine. You\\ncan install that like so:\\nllm install llm-gpt4all\\n2.1.5 API key management\\nManyLLMmodelsrequireanAPIkey. TheseAPIkeyscanbeprovidedtothistoolusingseveraldifferentmechanisms.\\nYou can obtain an API key for OpenAI’s language models from the API keys page on their site.\\nSaving and using stored keys\\nThe easiest way to store an API key is to use the llm keys set command:\\nllm keys set openai\\nYou will be prompted to enter the key like this:\\n% llm keys set openai\\nEnter key:\\nOnce stored, this key will be automatically used for subsequent calls to the API:\\nllm \"Five ludicrous names for a pet lobster\"\\nYou can list the names of keys that have been set using this command:\\nllm keys\\nKeysthatarestoredinthiswayliveinafilecalled keys.json . Thisfileislocatedatthepathshownwhenyourunthe\\nfollowing command:\\n6 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 12}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm keys path\\nOnmacOSthiswillbe ~/Library/Application Support/io.datasette.llm/keys.json . OnLinuxitmaybe\\nsomething like ~/.config/io.datasette.llm/keys.json .\\nPassing keys using the –key option\\nKeys can be passed directly using the --keyoption, like this:\\nllm \"Five names for pet weasels\" --key sk-my-key-goes-here\\nYoucanalsopassthealiasofakeystoredinthe keys.json file. Forexample,ifyouwanttomaintainapersonalAPI\\nkey you could add that like this:\\nllm keys set personal\\nAnd then use it for prompts like so:\\nllm \"Five friendly names for a pet skunk\" --key personal\\nKeys in environment variables\\nKeys can also be set using an environment variable. These are different for different models.\\nFor OpenAI models the key will be read from the OPENAI_API_KEY environment variable.\\nThe environment variable will be used if no --keyoption is passed to the command and there is not a key configured\\ninkeys.json\\nTo use an environment variable in place of the keys.json key run the prompt like this:\\nllm \\'my prompt \\'--key $OPENAI_API_KEY\\n2.1.6 Configuration\\nYou can configure LLM in a number of different ways.\\nSetting a custom default model\\nThe model used when calling llmwithout the -m/--model option defaults to gpt-4o-mini - the fastest and least\\nexpensive OpenAI model.\\nYou can use the llm models default command to set a different default model. For GPT-4o (slower and more\\nexpensive, but more capable) run this:\\nllm models default gpt-4o\\nYou can view the current model by running this:\\nllm models default\\nAny of the supported aliases for a model can be passed to this command.\\n2.1. Setup 7'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 13}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nSetting a custom directory location\\nThis tool stores various files - prompt templates, stored keys, preferences, a database of logs - in a directory on your\\ncomputer.\\nOn macOS this is ~/Library/Application Support/io.datasette.llm/ .\\nOn Linux it may be something like ~/.config/io.datasette.llm/ .\\nYou can set a custom location for this directory by setting the LLM_USER_PATH environment variable:\\nexport LLM_USER_PATH=/path/to/my/custom/directory\\nTurning SQLite logging on and off\\nBydefault,LLMwilllogeverypromptandresponseyoumaketoaSQLitedatabase-see LoggingtoSQLite formore\\ndetails.\\nYou can turn this behavior off by default by running:\\nllm logs off\\nOr turn it back on again with:\\nllm logs on\\nRunllm logs status to see the current states of the setting.\\n2.2 Usage\\nThe command to run a prompt is llm prompt 'your prompt '. This is the default command, so you can use llm\\n'your prompt 'as a shortcut.\\n2.2.1 Executing a prompt\\nThese examples use the default OpenAI gpt-3.5-turbo model, which requires you to first set an OpenAI API key .\\nYoucaninstallLLMplugins tousemodelsfromotherproviders,includingopenlylicensedmodelsyoucanrundirectly\\non your own computer.\\nTo run a prompt, streaming tokens as they come in:\\nllm 'Ten names for cheesecakes '\\nTo disable streaming and only return the response once it has completed:\\nllm 'Ten names for cheesecakes '--no-stream\\nTo switch from ChatGPT 3.5 (the default) to GPT-4o:\\nllm 'Ten names for cheesecakes '-m gpt-4o\\n8 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 14}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nYou can use -m 4oas an even shorter shortcut.\\nPass--model <model name> to use a different model. Run llm models to see a list of available models.\\nYou can also send a prompt to standard input, for example:\\necho 'Ten names for cheesecakes '| llm\\nIfyousendtexttostandardinputandprovidearguments,theresultingpromptwillconsistofthepipedcontentfollowed\\nby the arguments:\\ncat myscript.py | llm 'explain this code '\\nWill run a prompt of:\\n<contents of myscript.py> explain this code\\nFor models that support them, system prompts are a better tool for this kind of prompting.\\nSomemodelssupportoptions. Youcanpasstheseusing -o/--option name value -forexample,tosetthetemper-\\nature to 1.5 run this:\\nllm 'Ten names for cheesecakes '-o temperature 1.5\\n2.2.2 Completion prompts\\nSome models are completion models - rather than being tuned to respond to chat style prompts, they are designed to\\ncomplete a sentence or paragraph.\\nAn example of this is the gpt-3.5-turbo-instruct OpenAI model.\\nYou can prompt that model the same way as the chat models, but be aware that the prompt format that works best is\\nlikely to differ.\\nllm -m gpt-3.5-turbo-instruct 'Reasons to tame a wild beaver: '\\n2.2.3 Continuing a conversation\\nBy default, the tool will start a new conversation each time you run it.\\nYou can opt to continue the previous conversation by passing the -c/--continue option:\\nllm 'More names '-c\\nThis will re-send the prompts and responses for the previous conversation as part of the call to the language model.\\nNote that this can add up quickly in terms of tokens, especially if you are using expensive models.\\n--continue will automatically use the same model as the conversation that you are continuing, even if you omit the\\n-m/--model option.\\nTo continue a conversation that is not the most recent one, use the --cid/--conversation <id> option:\\nllm 'More names '--cid 01h53zma5txeby33t1kbe3xk8q\\nYou can find these conversation IDs using the llm logs command.\\n2.2. Usage 9\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 15}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.2.4 Using with a shell\\nTo learn more about your computer’s operating system based on the output of uname -a , run this:\\nllm \"Tell me about my operating system: $(uname -a )\"\\nThis pattern of using $(command) inside a double quoted string is a useful way to quickly assemble prompts.\\n2.2.5 System prompts\\nYou can use -s/--system \\'...\\'to set a system prompt.\\nllm \\'SQL to calculate total sales by month \\'\\\\\\n--system \\'You are an exaggerated sentient cheesecake that knows SQL and talks about␣\\n˓→cheesecake a lot \\'\\nThis is useful for piping content to standard input, for example:\\ncurl -s \\'https://simonwillison.net/2023/May/15/per-interpreter-gils/ \\'|\\\\\\nllm -s \\'Suggest topics for this post as a JSON array \\'\\nOr to generate a description of changes made to a Git repository since the last commit:\\ngit diff | llm -s \\'Describe these changes \\'\\nDifferent models support system prompts in different ways.\\nTheOpenAImodelsareparticularlygoodatusingsystempromptsasinstructionsforhowtheyshouldprocessadditional\\ninput sent as part of the regular prompt.\\nOther models might use system prompts change the default voice and attitude of the model.\\nSystem prompts can be saved as templates to create reusable tools. For example, you can create a template called\\npytestlike this:\\nllm -s \\'write pytest tests for this code \\'--save pytest\\nAnd then use the new template like this:\\ncat llm/utils.py | llm -t pytest\\nSeeprompt templates for more.\\n2.2.6 Starting an interactive chat\\nThellm chat command starts an ongoing interactive chat with a model.\\nThisisparticularly usefulformodelsthatrun onyourownmachine, sinceitsavesthem fromhavingtobeloaded into\\nmemory each time a new prompt is added to a conversation.\\nRunllm chat , optionally with a -m model_id , to start a chat conversation:\\nllm chat -m chatgpt\\n10 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 16}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nEach chat starts a new conversation. A record of each conversation can be accessed through the logs.\\nYou can pass -cto start a conversation as a continuation of your most recent prompt. This will automatically use the\\nmost recently used model:\\nllm chat -c\\nFor models that support them, you can pass options using -o/--option :\\nllm chat -m gpt-4 -o temperature 0.5\\nYou can pass a system prompt to be used for your chat conversation:\\nllm chat -m gpt-4 -s \\'You are a sentient cheesecake \\'\\nYou can also pass a template - useful for creating chat personas that you wish to return to.\\nHere’s how to create a template for your GPT-4 powered cheesecake:\\nllm --system \\'You are a sentient cheesecake \\'-m gpt-4 --save cheesecake\\nNow you can start a new chat with your cheesecake any time you like using this:\\nllm chat -t cheesecake\\nChatting with gpt-4\\nType \\'exit \\'or\\'quit \\'to exit\\nType \\'!multi \\'to enter multiple lines, then \\'!end \\'to finish\\n> who are you?\\nI am a sentient cheesecake, meaning I am an artificial\\nintelligence embodied in a dessert form, specifically a\\ncheesecake. However, I don \\'t consume or prepare foods\\nlike humans do, I communicate, learn and help answer\\nyour queries.\\nTypequitorexitfollowed by <enter> to end a chat session.\\nSometimes you may want to paste multiple lines of text into a chat at once - for example when debugging an error\\nmessage.\\nTo do that, type !multito start a multi-line input. Type or paste your text, then type !endand hit<enter> to finish.\\nIfyourpastedtextmightitselfcontaina !endline,youcansetacustomdelimiterusing !multi abc followedby !end\\nabcat the end:\\nChatting with gpt-4\\nType \\'exit \\'or\\'quit \\'to exit\\nType \\'!multi \\'to enter multiple lines, then \\'!end \\'to finish\\n> !multi custom-end\\nExplain this error:\\nFile \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣\\n˓→1391, in https_open\\nreturn self.do_open(http.client.HTTPSConnection, req,\\nFile \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/urllib/request.py\", line␣\\n˓→1351, in do_open\\nraise URLError(err)\\n(continues on next page)\\n2.2. Usage 11'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 17}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nurllib.error.URLError: <urlopen error [Errno 8] nodename nor servname provided, or not␣\\n˓→known>\\n!end custom-end\\n2.2.7 Listing available models\\nThellm models command lists every model that can be used with LLM, along with their aliases. This includes\\nmodels that have been installed using plugins.\\nllm models\\nExample output:\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nPaLM 2: chat-bison-001 (aliases: palm, palm2)\\nAdd--options to also see documentation for the options supported by each model:\\nllm models --options\\nOutput:\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\ntemperature: float\\nWhat sampling temperature to use, between 0 and2. Higher values like\\n0.8 will make the output more random, whilelower values like 0.2 will\\nmake it more focused anddeterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.\\ntop_p: float\\nAn alternative to sampling withtemperature, called nucleus sampling,\\nwhere the model considers the results of the tokens withtop_p\\nprobability mass. So 0.1 means only the tokens comprising the top 10%\\nprobability mass are considered. Recommended to use top_p or\\ntemperature but notboth.\\nfrequency_penalty: float\\nNumber between -2.0 and2.0. Positive values penalize new tokens based\\non their existing frequency inthe text so far, decreasing the model 's\\nlikelihood to repeat the same line verbatim.\\npresence_penalty: float\\nNumber between -2.0 and2.0. Positive values penalize new tokens based\\non whether they appear inthe text so far, increasing the model 's\\nlikelihood to talk about new topics.\\nstop: str\\nA string where the API will stop generating further tokens.\\nlogit_bias: dict, str\\nModify the likelihood of specified tokens appearing inthe completion.\\n(continues on next page)\\n12 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 18}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nPass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100} \\'\\nseed: int\\nInteger seed to attempt to sample deterministically\\njson_object: boolean\\nOutput a valid JSON object {...}. Prompt must mention JSON.\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4-1106-preview\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4-0125-preview\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\n(continues on next page)\\n2.2. Usage 13'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 19}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4o (aliases: 4o)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Chat: o1-preview\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\n(continues on next page)\\n14 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 20}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\njson_object: boolean\\nOpenAI Chat: o1-mini\\ntemperature: float\\nmax_tokens: int\\ntop_p: float\\nfrequency_penalty: float\\npresence_penalty: float\\nstop: str\\nlogit_bias: dict, str\\nseed: int\\njson_object: boolean\\nOpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\ntemperature: float\\nWhat sampling temperature to use, between 0 and2. Higher values like\\n0.8 will make the output more random, whilelower values like 0.2 will\\nmake it more focused anddeterministic.\\nmax_tokens: int\\nMaximum number of tokens to generate.\\ntop_p: float\\nAn alternative to sampling withtemperature, called nucleus sampling,\\nwhere the model considers the results of the tokens withtop_p\\nprobability mass. So 0.1 means only the tokens comprising the top 10%\\nprobability mass are considered. Recommended to use top_p or\\ntemperature but notboth.\\nfrequency_penalty: float\\nNumber between -2.0 and2.0. Positive values penalize new tokens based\\non their existing frequency inthe text so far, decreasing the model \\'s\\nlikelihood to repeat the same line verbatim.\\npresence_penalty: float\\nNumber between -2.0 and2.0. Positive values penalize new tokens based\\non whether they appear inthe text so far, increasing the model \\'s\\nlikelihood to talk about new topics.\\nstop: str\\nA string where the API will stop generating further tokens.\\nlogit_bias: dict, str\\nModify the likelihood of specified tokens appearing inthe completion.\\nPass a JSON string like \\'{\"1712\":-100, \"892\":-100, \"1489\":-100} \\'\\nseed: int\\nInteger seed to attempt to sample deterministically\\nlogprobs: int\\nInclude the log probabilities of most likely N per token\\nWhen running a prompt you can pass the full model name or any of the aliases to the -m/--model option:\\nllm -m 4o \\\\\\n\\'As many names for cheesecakes as you can think of, with detailed descriptions \\'\\n2.2. Usage 15'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 21}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.3 OpenAI models\\nLLM ships with a default plugin for talking to OpenAI’s API. OpenAI offer both language models and embedding\\nmodels, and LLM can access both types.\\n2.3.1 Configuration\\nAll OpenAI models are accessed using an API key. You can obtain one from the API keys page on their site.\\nOnce you have created a key, configure LLM to use it by running:\\nllm keys set openai\\nThen paste in the API key.\\n2.3.2 OpenAI language models\\nRunllm models for a full list of available models. The OpenAI models supported by LLM are:\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat: gpt-4-1106-preview\\nOpenAI Chat: gpt-4-0125-preview\\nOpenAI Chat: gpt-4-turbo-2024-04-09\\nOpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\\nOpenAI Chat: gpt-4o (aliases: 4o)\\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\\nOpenAI Chat: o1-preview\\nOpenAI Chat: o1-mini\\nOpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\\nSee the OpenAI models documentation for details of each of these.\\ngpt-4o-mini (aliased to 4o-mini) is the least expensive model, and is the default for if you don’t specify a model at\\nall.gpt-4o(aliased to 4o) is the newest, cheapest and fastest of the GPT-4 family of models.\\nThegpt-3.5-turbo-instruct modelisalittledifferent-itisacompletionmodelratherthanachatmodel,described\\nin the OpenAI completions documentation.\\nCompletion models can be called with the -o logprobs 3 option (not supported by chat models) which will cause\\nLLM to store 3 log probabilities for each returned token in the SQLite database. Consult this issue for details on how\\nto read these values.\\n16 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 22}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.3.3 OpenAI embedding models\\nRunllm embed-models foralistof embeddingmodels . ThefollowingOpenAIembeddingmodelsaresupportedby\\nLLM:\\nada-002 (aliases: ada, oai)\\n3-small\\n3-large\\n3-small-512\\n3-large-256\\n3-large-1024\\nThe3-small modeliscurrentlythemostinexpensive. 3-large costsmorebutismorecapable-seeNewembedding\\nmodels and API updates on the OpenAI blog for details and benchmarks.\\nAn important characteristic of any embedding model is the size of the vector it returns. Smaller vectors cost less to\\nstore and query, but may be less accurate.\\nOpenAI3-small and3-large vectorscanbesafelytruncatedtolowerdimensionswithoutlosingtoomuchaccuracy.\\nThe-intmodels provided by LLM are pre-configured to do this, so 3-large-256 is the3-large model truncated\\nto 256 dimensions.\\nThe vector size of the supported OpenAI embedding models are as follows:\\nModel Size\\nada-002 1536\\n3-small 1536\\n3-large 3072\\n3-small-512 512\\n3-large-256 256\\n3-large-1024 1024\\n2.3.4 Adding more OpenAI models\\nOpenAI occasionally release new models with new names. LLM aims to ship new releases to support these, but you\\ncan also configure them directly, by adding them to a extra-openai-models.yaml configuration file.\\nRun this command to find the directory in which this file should be created:\\ndirname \" $(llm logs path )\"\\nOn my Mac laptop I get this:\\n~/Library/Application Support/io.datasette.llm\\nCreate a file in that directory called extra-openai-models.yaml .\\nLet’s say OpenAI have just released the gpt-3.5-turbo-0613 model and you want to use it, despite LLM not yet\\nshipping support. You could configure that by adding this to the file:\\n-model_id : gpt-3.5-turbo-0613\\naliases: [\"0613\"]\\nThemodel_id is the identifier that will be recorded in the LLM logs. You can use this to specify the model, or you\\ncan optionally include a list of aliases for that model.\\n2.3. OpenAI models 17'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 23}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nIf the model is a completion model (such as gpt-3.5-turbo-instruct ) addcompletion: true to the configu-\\nration.\\nWith this configuration in place, the following command should run a prompt against the new model:\\nllm -m 0613 'What is the capital of France? '\\nRunllm models to confirm that the new model is now available:\\nllm models\\nExample output:\\nOpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\\nOpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\\nOpenAI Chat: gpt-4-32k (aliases: 4-32k)\\nOpenAI Chat: gpt-3.5-turbo-0613 (aliases: 0613)\\nRunningllm logs -n 1 should confirm that the prompt and response has been correctly logged to the database.\\n2.4 Other models\\nLLM supports OpenAI models by default. You can install pluginsto add support for other models. You can also add\\nadditional OpenAI-API-compatible models using a configuration file .\\n2.4.1 Installing and using a local model\\nLLM plugins can provide local models that run on your machine.\\nTo install llm-gpt4all , providing 17 models from the GPT4All project, run this:\\nllm install llm-gpt4all\\nRunllm models to see the expanded list of available models.\\nTo run a prompt through one of the models from GPT4All specify it using -m/--model :\\nllm -m orca-mini-3b-gguf2-q4_0 'What is the capital of France? '\\nThe model will be downloaded and cached the first time you use it.\\nCheck the plugin directory for the latest list of available plugins for other models.\\n2.4.2 OpenAI-compatible models\\nProjectssuchasLocalAIofferaRESTAPIthatimitatestheOpenAIAPIbutcanbeusedtorunothermodels,including\\nmodels that can be installed on your own machine. These can be added using the same configuration mechanism.\\nThemodel_id is the name LLM will use for the model. The model_name is the name which needs to be passed to\\nthe API - this might differ from the model_id , especially if the model_id could potentially clash with other installed\\nmodels.\\nTheapi_base key can be used to point the OpenAI client library at a different API endpoint.\\n18 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 24}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nToaddthe orca-mini-3b modelhostedbyalocalinstallationofLocalAI,addthistoyour extra-openai-models.\\nyamlfile:\\n-model_id : orca-openai-compat\\nmodel_name : orca-mini-3b.ggmlv3\\napi_base : \"http://localhost:8080\"\\nIf theapi_base is set, the existing configured openaiAPI key will not be sent by default.\\nYou can set api_key_name to the name of a key stored using the API key management feature.\\nAddcompletion: true if the model is a completion model that uses a /completion as opposed to a /\\ncompletion/chat endpoint.\\nHaving configured the model like this, run llm models to check that it installed correctly. You can then run prompts\\nagainst it like so:\\nllm -m orca-openai-compat \\'What is the capital of France? \\'\\nAnd confirm they were logged correctly with:\\nllm logs -n 1\\nExtra HTTP headers\\nSomeproviderssuchasopenrouter.aimayrequirethesettingofadditionalHTTPheaders. Youcansetthoseusingthe\\nheaders: key like this:\\n-model_id : claude\\nmodel_name : anthropic/claude-2\\napi_base : \"https://openrouter.ai/api/v1\"\\napi_key_name : openrouter\\nheaders:\\nHTTP-Referer : \"https://llm.datasette.io/\"\\nX-Title: LLM\\n2.5 Embeddings\\nEmbeddingmodelsallowyoutotakeapieceoftext-aword,sentence,paragraphorevenawholearticle,andconvert\\nthat into an array of floating point numbers.\\nThis floating point array is called an “embedding vector”, and works as a numerical representation of the semantic\\nmeaning of the content in a many-multi-dimensional space.\\nBy calculating the distance between embedding vectors, we can identify which content is semantically “nearest” to\\nother content.\\nThis can be used to build features like related article lookups. It can also be used to build semantic search, where a\\nuser can search for a phrase and get back results that are semantically similar to that phrase even if they do not share\\nany exact keywords.\\nSomeembeddingmodelslikeCLIPcanevenworkagainstbinaryfilessuchasimages. Thesecanbeusedtosearchfor\\nimages that are similar to other images, or to search for images that are semantically similar to a piece of text.\\n2.5. Embeddings 19'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 25}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nLLM supports multiple embedding models through plugins. Once installed, an embedding model can be used on the\\ncommand-line or via the Python API to calculate and store embeddings for content, and then to perform similarity\\nsearches against those embeddings.\\nSee LLM now provides tools for working with embeddings for an extended explanation of embeddings, why they are\\nuseful and what you can do with them.\\n2.5.1 Embedding with the CLI\\nLLM provides command-line utilities for calculating and storing embeddings for pieces of content.\\nllm embed\\nThellm embed command can be used to calculate embedding vectors for a string of content. These can be returned\\ndirectly to the terminal, stored in a SQLite database, or both.\\nReturning embeddings to the terminal\\nThe simplest way to use this command is to pass content to it using the -c/--content option, like this:\\nllm embed -c 'This is some content '-m 3-small\\n-m 3-small specifiestheOpenAI text-embedding-3-small model. YouwillneedtohavesetanOpenAIAPIkey\\nusingllm keys set openai for this to work.\\nYou can install plugins to access other models. The llm-sentence-transformers plugin can be used to run models on\\nyour own laptop, such as the MiniLM-L6 model:\\nllm install llm-sentence-transformers\\nllm embed -c 'This is some content '-m sentence-transformers/all-MiniLM-L6-v2\\nThellm embed command returns a JSON array of floating point numbers directly to the terminal:\\n[0.123, 0.456, 0.789...]\\nYou can omit the -m/--model option if you set a default embedding model .\\nLLM also offers a binary storage format for embeddings, described in embeddings storage format .\\nYoucanoutputembeddingsusingthatformatasrawbytesusing --format blob ,orinhexadecimalusing --format\\nhex, or in Base64 using --format base64 :\\nllm embed -c 'This is some content '-m 3-small --format base64\\nThis outputs:\\n8NGzPFtdgTqHcZw7aUT6u+++WrwwpZo8XbSxv...\\nSome models such as llm-clip can run against binary data. You can pass in binary data using the -iand--binary\\noptions:\\nllm embed --binary -m clip -i image.jpg\\nOr from standard input like this:\\n20 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 26}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\ncat image.jpg | llm embed --binary -m clip -i -\\nStoring embeddings in SQLite\\nEmbeddingsaremuchmoreusefulifyoustorethemsomewhere,soyoucancalculatesimilarityscoresbetweendifferent\\nembeddings later on.\\nLLM includes the concept of a collection of embeddings. A collection groups together a set of stored embeddings\\ncreated using the same model, each with a unique ID within that collection.\\nEmbeddings also store a hash of the content that was embedded. This hash is later used to avoid calculating duplicate\\nembeddings for the same content.\\nFirst, we’ll set a default model so we don’t have to keep repeating it:\\nllm embed-models default 3-small\\nThellm embed command can store results directly in a named collection like this:\\nllm embed quotations philkarlton-1 -c \\\\\\n'There are only two hard things in Computer Science: cache invalidation and naming␣\\n˓→things '\\nThis stores the given text in the quotations collection under the key philkarlton-1 .\\nYou can also pipe content to standard input, like this:\\ncat one.txt | llm embed files one\\nThis will store the embedding for the contents of one.txt in thefilescollection under the key one.\\nA collection will be created the first time you mention it.\\nCollections have a fixed embedding model, which is the model that was used for the first embedding stored in that\\ncollection.\\nIn the above example this would have been the default embedding model at the time that the command was run.\\nThe following example stores the embedding for the string “my happy hound” in a collection called phrases under\\nthe keyhoundand using the model 3-small:\\nllm embed phrases hound -m 3-small -c 'my happy hound '\\nBydefault,theSQLitedatabaseusedtostoreembeddingsisthe embeddings.db intheusercontentdirectorymanaged\\nby LLM.\\nYou can see the path to this directory by running llm collections path .\\nYou can store embeddings in a different SQLite database by passing a path to it using the -d/--database option to\\nllm embed . If this file does not exist yet the command will create it:\\nllm embed phrases hound -d my-embeddings.db -c 'my happy hound '\\nThis creates a database file called my-embeddings.db in the current directory.\\n2.5. Embeddings 21\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 27}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nStoring content and metadata\\nBy default, only the entry ID and the embedding vector are stored in the database table.\\nYou can store a copy of the original text in the content column by passing the --store option:\\nllm embed phrases hound -c \\'my happy hound \\'--store\\nYoucanalsostoreaJSONobjectcontainingarbitrarymetadatainthe metadata columnbypassingthe --metadata\\noption. This example uses both --store and--metadata options:\\nllm embed phrases hound \\\\\\n-m 3-small \\\\\\n-c\\'my happy hound \\'\\\\\\n--metadata \\'{\"name\": \"Hound\"} \\'\\\\\\n--store\\nData stored in this way will be returned by calls to llm similar , for example:\\nllm similar phrases -c \\'hound \\'\\n{\"id\": \"hound\", \"score\": 0.8484683588631485, \"content\": \"my happy hound\", \"metadata\": {\\n˓→\"name\": \"Hound\"}}\\nllm embed-multi\\nThellm embed command embeds a single string at a time.\\nllm embed-multi can be used to embed multiple strings at once, taking advantage of any efficiencies that the em-\\nbedding model may provide when processing multiple strings.\\nThis command can be called in one of three ways:\\n1. With a CSV, TSV, JSON or newline-delimited JSON file\\n2. With a SQLite database and a SQL query\\n3. With one or more paths to directories, each accompanied by a glob pattern\\nAll three mechanisms support these options:\\n•-m model_id to specify the embedding model to use\\n•-d database.db to specify a different database file to store the embeddings in\\n•--store to store the original content in the embeddings table in addition to the embedding vector\\n•--prefix to prepend a prefix to the stored ID of each item\\n•--batch-size SIZE to process embeddings in batches of the specified size\\n22 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 28}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nEmbedding data from a CSV, TSV or JSON file\\nYoucanembeddatafromaCSV,TSVorJSONfilebypassingthatfiletothecommandasthesecondoption,afterthe\\ncollection name.\\nYourfilemustcontainatleasttwocolumns. ThefirstoneisexpectedtocontaintheIDoftheitem,andanysubsequent\\ncolumns will be treated as containing content to be embedded.\\nAn example CSV file might look like this:\\nid,content\\none,This isthe first item\\ntwo,This isthe second item\\nTSV would use tabs instead of commas.\\nJSON files can be structured like this:\\n[\\n{\"id\": \"one\", \"content\" : \"This is the first item\"},\\n{\"id\": \"two\", \"content\" : \"This is the second item\"}\\n]\\nOr as newline-delimited JSON like this:\\n{\"id\": \"one\", \"content\" : \"This is the first item\"}\\n{\"id\": \"two\", \"content\" : \"This is the second item\"}\\nIn each of these cases the file can be passed to llm embed-multi like this:\\nllm embed-multi items mydata.csv\\nThe first argument is the name of the collection, the second is the filename.\\nYou can also pipe content to standard input of the tool using -:\\ncat mydata.json | llm embed-multi items -\\nLLMwillattempttodetecttheformatofyourdataautomatically. Ifthisdoesn’tworkyoucanspecifytheformatusing\\nthe--format option. This is required if you are piping newline-delimited JSON to standard input.\\ncat mydata.json | llm embed-multi items - --format nl\\nOther supported --format options are csv,tsvandjson.\\nThis example embeds the data from a JSON file in a collection called itemsin database called docs.db using the\\n3-small modelandstorestheoriginalcontentinthe embeddings tableaswell,addingaprefixof my-items/ toeach\\nID:\\nllm embed-multi items mydata.json \\\\\\n-d docs.db \\\\\\n-m 3-small \\\\\\n--prefix my-items/ \\\\\\n--store\\n2.5. Embeddings 23'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 29}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nEmbedding data from a SQLite database\\nYoucanembeddatafromaSQLitedatabaseusing --sql,optionallycombinedwith --attach toattachanadditional\\ndatabase.\\nIf you are storing embeddings in the same database as the source data, you can do this:\\nllm embed-multi docs \\\\\\n-d docs.db \\\\\\n--sql 'select id, title, content from documents '\\\\\\n-m 3-small\\nThedocs.db database here contains a documents table, and we want to embed the titleandcontent columns\\nfrom that table and store the results back in the same database.\\nTo load content from a database other than the one you are using to store embeddings, attach it with the --attach\\noption and use alias.table in your SQLite query:\\nllm embed-multi docs \\\\\\n-d embeddings.db \\\\\\n--attach other other.db \\\\\\n--sql 'select id, title, content from other.documents '\\\\\\n-m 3-small\\nEmbedding data from files in directories\\nLLM can embed the content of every text file in a specified directory, using the file’s path and name as the ID.\\nConsider a directory structure like this:\\ndocs/aliases.md\\ndocs/contributing.md\\ndocs/embeddings/binary.md\\ndocs/embeddings/cli.md\\ndocs/embeddings/index.md\\ndocs/index.md\\ndocs/logging.md\\ndocs/plugins/directory.md\\ndocs/plugins/index.md\\nTo embed all of those documents, you can run the following:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs '**/*.md '\\\\\\n-d documentation.db \\\\\\n--store\\nHere--files docs '**/*.md 'specifiesthatthe docsdirectoryshouldbescannedforfilesmatchingthe **/*.md\\nglob pattern - which will match Markdown files in any nested directory.\\nThe result of the above command is a embeddings table with the following IDs:\\n24 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 30}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\naliases.md\\ncontributing.md\\nembeddings/binary.md\\nembeddings/cli.md\\nembeddings/index.md\\nindex.md\\nlogging.md\\nplugins/directory.md\\nplugins/index.md\\nEach corresponding to embedded content for the file in question.\\nThe--prefix option can be used to add a prefix to each ID:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs '**/*.md '\\\\\\n-d documentation.db \\\\\\n--store \\\\\\n--prefix llm-docs/\\nThis will result in the following IDs instead:\\nllm-docs/aliases.md\\nllm-docs/contributing.md\\nllm-docs/embeddings/binary.md\\nllm-docs/embeddings/cli.md\\nllm-docs/embeddings/index.md\\nllm-docs/index.md\\nllm-docs/logging.md\\nllm-docs/plugins/directory.md\\nllm-docs/plugins/index.md\\nFilesareassumedtobe utf-8,butLLMwillfallbackto latin-1 ifitencountersanencodingerror. Youcanspecify\\na different set of encodings using the --encoding option.\\nThis example will try utf-16first and then mac_roman before falling back to latin-1:\\nllm embed-multi documentation \\\\\\n-m 3-small \\\\\\n--files docs '**/*.md '\\\\\\n-d documentation.db \\\\\\n--encoding utf-16 \\\\\\n--encoding mac_roman \\\\\\n--encoding latin-1\\nIf a file cannot be read it will be logged to standard error but the script will keep on running.\\nIf you are embedding binary content such as images for use with CLIP, add the --binary option:\\nllm embed-multi photos \\\\\\n-m clip \\\\\\n--files photos/ '*.jpeg '--binary\\n2.5. Embeddings 25\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 31}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm similar\\nThellm similar commandsearchesacollectionofembeddingsfortheitemsthataremostsimilartoagivenoritem\\nID.\\nThiscurrentlyusesaslowbrute-forceapproachwhichdoesnotscalewelltolargecollections. Seeissue216forplans\\nto add a more scalable approach via vector indexes provided by plugins.\\nTo search the quotations collection for items that are semantically similar to \\'computer science \\':\\nllm similar quotations -c \\'computer science \\'\\nThis embeds the provided string and returns a newline-delimited list of JSON objects like this:\\n{\"id\": \"philkarlton-1\", \"score\": 0.8323904531677017, \"content\" :null,\"metadata\" :null}\\nYou can compare against text stored in a file using -i filename :\\nllm similar quotations -i one.txt\\nOr feed text to standard input using -i -:\\necho \\'computer science \\'| llm similar quotations -i -\\nWhen using a model like CLIP, you can find images similar to an input image using -i filename with--binary :\\nllm similar photos -i image.jpg --binary\\nllm embed-models\\nTo list all available embedding models, including those provided by plugins, run this command:\\nllm embed-models\\nThe output should look something like this:\\n3-small (aliases: ada)\\nsentence-transformers/all-MiniLM-L6-v2 (aliases: all-MiniLM-L6-v2)\\nllm embed-models default\\nThis command can be used to get and set the default embedding model.\\nThis will return the name of the current default model:\\nllm embed-models default\\nYou can set a different default like this:\\nllm embed-models default 3-small\\nThis will set the default model to OpenAI’s 3-small model.\\nAny of the supported aliases for a model can be passed to this command.\\nYou can unset the default model using --remove-default :\\n26 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 32}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm embed-models default --remove-default\\nWhennodefaultmodelisset,the llm embed andllm embed-multi commandswillrequirethatamodelisspecified\\nusing-m/--model .\\nllm collections list\\nTo list all of the collections in the embeddings database, run this command:\\nllm collections list\\nAdd--jsonfor JSON output:\\nllm collections list --json\\nAdd-d/--database to specify a different database file:\\nllm collections list -d my-embeddings.db\\nllm collections delete\\nTo delete a collection from the database, run this:\\nllm collections delete collection-name\\nPass-dto specify a different database file:\\nllm collections delete collection-name -d my-embeddings.db\\n2.5.2 Using embeddings from Python\\nYou can load an embedding model using its model ID or alias like this:\\nimport llm\\nembedding_model = llm.get_embedding_model(\"3-small\")\\nTo embed a string, returning a Python list of floating point numbers, use the .embed() method:\\nvector = embedding_model.embed(\"my happy hound\")\\nIftheembeddingmodelcanhandlebinaryinput,youcancall .embed() withabytestringinstead. Youcancheckthe\\nsupports_binary property to see if this is supported:\\nifembedding_model.supports_binary:\\nvector = embedding_model.embed(open(\"my-image.jpg\", \"rb\").read())\\nTheembedding_model.supports_text property indicates if the model supports text input.\\nMany embeddings models are more efficient when you embed multiple strings or binary strings at once. To embed\\nmultiple strings at once, use the .embed_multi() method:\\n2.5. Embeddings 27'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 33}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nvectors = list(embedding_model.embed_multi([\"my happy hound\", \"my dissatisfied cat\"]))\\nThis returns a generator that yields one embedding vector per string.\\nEmbeddings are calculated in batches. By default all items will be processed in a single batch, unless the underlying\\nembedding model has defined its own preferred batch size. You can pass a custom batch size using batch_size=N ,\\nfor example:\\nvectors = list(embedding_model.embed_multi(lines_from_file, batch_size=20))\\nWorking with collections\\nThellm.Collection class can be used to work with collections of embeddings from Python code.\\nA collection is a named group of embedding vectors, each stored along with their IDs in a SQLite database table.\\nToworkwithembeddingsinthiswayyouwillneedaninstanceofasqlite-utilsDatabaseobject. Youcanthenpassthat\\nto thellm.Collection constructor along with the unique string name of the collection and the ID of the embedding\\nmodel you will be using with that collection:\\nimport sqlite_utils\\nimport llm\\n# This collection will use an in-memory database that will be\\n# discarded when the Python process exits\\ncollection = llm.Collection(\"entries\", model_id=\"3-small\")\\n# Or you can persist the database to disk like this:\\ndb = sqlite_utils.Database(\"my-embeddings.db\")\\ncollection = llm.Collection(\"entries\", db, model_id=\"3-small\")\\n# You can pass a model directly using model= instead of model_id=\\nembedding_model = llm.get_embedding_model(\"3-small\")\\ncollection = llm.Collection(\"entries\", db, model=embedding_model)\\nIf the collection already exists in the database you can omit the modelormodel_id argument - the model ID will be\\nread from the collections table.\\nTo embed a single string and store it in the collection, use the embed() method:\\ncollection.embed(\"hound\", \"my happy hound\")\\nThis stores the embedding for the string “my happy hound” in the entries collection under the key hound.\\nAddstore=True to store the text content itself in the database table along with the embedding vector.\\nTo attach additional metadata to an item, pass a JSON-compatible dictionary as the metadata= argument:\\ncollection.embed(\"hound\", \"my happy hound\", metadata={\"name\": \"Hound\"}, store= True)\\nThis additional metadata will be stored as JSON in the metadata column of the embeddings database table.\\n28 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 34}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nStoring embeddings in bulk\\nThecollection.embed_multi() method can be used to store embeddings for multiple items at once. This can be\\nmore efficient for some embedding models.\\ncollection.embed_multi(\\n[\\n(\"hound\", \"my happy hound\"),\\n(\"cat\", \"my dissatisfied cat\"),\\n],\\n# Add this to store the strings in the content column:\\nstore= True,\\n)\\nTo include metadata to be stored with each item, call embed_multi_with_metadata() :\\ncollection.embed_multi_with_metadata(\\n[\\n(\"hound\", \"my happy hound\", {\"name\": \"Hound\"}),\\n(\"cat\", \"my dissatisfied cat\", {\"name\": \"Cat\"}),\\n],\\n# This can also take the store=True argument:\\nstore= True,\\n)\\nThebatch_size= argumentdefaultsto100,andwillbeusedunlesstheembeddingmodelitselfdefinesalowerbatch\\nsize. You can adjust this if you are having trouble with memory while embedding large collections:\\ncollection.embed_multi(\\n(\\n(i, line)\\nfori, line inenumerate(lines_in_file)\\n),\\nbatch_size=10\\n)\\nCollection class reference\\nA collection instance has the following properties and methods:\\n•id- the integer ID of the collection in the database\\n•name- the string name of the collection (unique in the database)\\n•model_id - the string ID of the embedding model used for this collection\\n•model() - returns the EmbeddingModel instance, based on that model_id\\n•count() - returns the integer number of items in the collection\\n•embed(id: str, text: str, metadata: dict=None, store: bool=False) - embeds the given\\nstring and stores it in the collection under the given ID. Can optionally include metadata (stored as JSON) and\\nstore the text content itself in the database table.\\n•embed_multi(entries: Iterable, store: bool=False, batch_size: int=100) - see above\\n2.5. Embeddings 29'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 35}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n•embed_multi_with_metadata(entries: Iterable, store: bool=False, batch_size:\\nint=100) - see above\\n•similar(query: str, number: int=10) -returnsalistofentriesthataremostsimilartotheembedding\\nof the given query string\\n•similar_by_id(id: str, number: int=10) -returnsalistofentriesthataremostsimilartotheembed-\\nding of the item with the given ID\\n•similar_by_vector(vector: List[float], number: int=10, skip_id: str=None) - returns a\\nlist of entries that are most similar to the given embedding vector, optionally skipping the entry with the given\\nID\\n•delete() - deletes the collection and its embeddings from the database\\nThere is also a Collection.exists(db, name) class method which returns a boolean value and can be used to\\ndetermine if a collection exists or not in a database:\\nifCollection.exists(db, \"entries\"):\\nprint(\"The entries collection exists\")\\nRetrieving similar items\\nOnceyouhavepopulatedacollectionofembeddingsyoucanretrievetheentriesthataremostsimilartoagivenstring\\nusing thesimilar() method.\\nThis method uses a brute force approach, calculating distance scores against every document. This is fine for small\\ncollections, but will not scale to large collections. See issue 216 for plans to add a more scalable approach via vector\\nindexes provided by plugins.\\nforentry incollection.similar(\"hound\"):\\nprint(entry.id, entry.score)\\nThe string will first by embedded using the model for the collection.\\nTheentryobject returned is an object with the following properties:\\n•id- the string ID of the item\\n•score- the floating point similarity score between the item and the query string\\n•content - the string text content of the item, if it was stored - or None\\n•metadata - the dictionary (from JSON) metadata for the item, if it was stored - or None\\nThis defaults to returning the 10 most similar items. You can change this by passing a different number= argument:\\nforentry incollection.similar(\"hound\", number=5):\\nprint(entry.id, entry.score)\\nThesimilar_by_id() methodtakestheIDofanotheriteminthecollectionandreturnsthemostsimilaritemstothat\\none, based on the embedding that has already been stored for it:\\nforentry incollection.similar_by_id(\"cat\"):\\nprint(entry.id, entry.score)\\nThe item itself is excluded from the results.\\n30 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 36}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nSQL schema\\nHere’s the SQL schema used by the embeddings database:\\nCREATE TABLE [collections] (\\n[id] INTEGER PRIMARY KEY ,\\n[name] TEXT,\\n[model] TEXT\\n)\\nCREATE TABLE \"embeddings\" (\\n[collection_id] INTEGER REFERENCES [collections]([id]),\\n[id] TEXT,\\n[embedding] BLOB,\\n[content] TEXT,\\n[content_blob] BLOB,\\n[content_hash] BLOB,\\n[metadata] TEXT,\\n[updated] INTEGER,\\nPRIMARY KEY ([collection_id], [id])\\n)\\n2.5.3 Writing plugins to add new embedding models\\nRead the plugin tutorial for details on how to develop and package a plugin.\\nThis page shows an example plugin that implements and registers a new embedding model.\\nThere are two components to an embedding model plugin:\\n1. Animplementationofthe register_embedding_models() hook,whichtakesa register callbackfunction\\nand calls it to register the new model with the LLM plugin system.\\n2. A class that extends the llm.EmbeddingModel abstract base class.\\nTheonlyrequiredmethodonthisclassis embed_batch(texts) ,whichtakesaniterableofstringsandreturns\\nan iterator over lists of floating point numbers.\\nThefollowingexampleusesthesentence-transformerspackagetoprovideaccesstotheMiniLM-L6embeddingmodel.\\nimport llm\\nfrom sentence_transformers import SentenceTransformer\\n@llm.hookimpl\\ndefregister_embedding_models(register):\\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\"\\nregister(SentenceTransformerModel(model_id, model_id), aliases=(\"all-MiniLM-L6-v2\",))\\nclass SentenceTransformerModel (llm.EmbeddingModel):\\ndef__init__(self, model_id, model_name):\\nself.model_id = model_id\\nself.model_name = model_name\\nself._model = None\\n(continues on next page)\\n2.5. Embeddings 31'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 37}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\ndefembed_batch(self, texts):\\nifself._model is None:\\nself._model = SentenceTransformer(self.model_name)\\nresults = self._model.encode(texts)\\nreturn(list(map(float, result)) forresult inresults)\\nOnce installed, the model provided by this plugin can be used with the llm embed command like this:\\ncat file.txt | llm embed -m sentence-transformers/all-MiniLM-L6-v2\\nOr via its registered alias like this:\\ncat file.txt | llm embed -m all-MiniLM-L6-v2\\nllm-sentence-transformers is a complete example of a plugin that provides an embedding model.\\nExecute Jina embeddings with a CLI using llm-embed-jina talks through a similar process to add support for the Jina\\nembeddings models.\\nEmbedding binary content\\nIf your model can embed binary content, use the supports_binary property to indicate that:\\nclass ClipEmbeddingModel (llm.EmbeddingModel):\\nmodel_id = \"clip\"\\nsupports_binary = True\\nsupports_text= True\\nsupports_text defaults to Trueand so is not necessary here. You can set it to Falseif your model only supports\\nbinary data.\\nIf your model accepts binary, your .embed_batch() model may be called with a list of Python bytestrings. These\\nmay be mixed with regular strings if the model accepts both types of input.\\nllm-clip is an example of a model that can embed both binary and text content.\\n2.5.4 Embedding storage format\\nThe default output format of the llm embed command is a JSON array of floating point numbers.\\nLLM stores embeddings in space-efficient format: a little-endian binary sequences of 32-bit floating point numbers,\\neach represented using 4 bytes.\\nThese are stored in a BLOBcolumn in a SQLite database.\\nThe following Python functions can be used to convert between this format and an array of floating point numbers:\\nimport struct\\ndefencode(values):\\nreturnstruct.pack(\"<\" + \"f\" * len(values), *values)\\ndefdecode(binary):\\nreturnstruct.unpack(\"<\" + \"f\" * (len(binary) // 4), binary)\\n32 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 38}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nThese functions are available as llm.encode() andllm.decode() .\\nIf you are using NumPy you can decode one of these binary values like this:\\nimport numpy as np\\nnumpy_array = np.frombuffer(value, \"<f4\")\\nThe<f4format string here ensures NumPy will treat the data as a little-endian sequence of 32-bit floats.\\n2.6 Plugins\\nLLMpluginscanenhanceLLMbymakingalternativeLargeLanguageModelsavailable,eitherviaAPIorbyrunning\\nthe models locally on your machine.\\nPlugins can also add new commands to the llmCLI tool.\\nTheplugin directory lists available plugins that you can install and use.\\nWriting a plugin to support a new model describes how to build a new plugin in detail.\\n2.6.1 Installing plugins\\nPlugins must be installed in the same virtual environment as LLM itself.\\nYou can find names of plugins to install in the plugin directory\\nUsethellm install command(athinwrapperaround pip install )toinstallpluginsinthecorrectenvironment:\\nllm install llm-gpt4all\\nPlugins can be uninstalled with llm uninstall :\\nllm uninstall llm-gpt4all -y\\nThe-yflag skips asking for confirmation.\\nYou can see additional models that have been added by plugins by running:\\nllm models\\nOr add--options to include details of the options available for each model:\\nllm models --options\\nTo run a prompt against a newly installed model, pass its name as the -m/--model option:\\nllm -m orca-mini-3b-gguf2-q4_0 \\'What is the capital of France? \\'\\n2.6. Plugins 33'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 39}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nListing installed plugins\\nRunllm plugins to list installed plugins:\\nllm plugins\\n[\\n{\\n\"name\": \"llm-mpt30b\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n],\\n\"version\" : \"0.1\"\\n},\\n{\\n\"name\": \"llm-palm\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n],\\n\"version\" : \"0.1\"\\n},\\n{\\n\"name\": \"llm.default_plugins.openai_models\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n]\\n},\\n{\\n\"name\": \"llm-gpt4all\",\\n\"hooks\": [\\n\"register_models\"\\n],\\n\"version\" : \"0.1\"\\n}\\n]\\nRunning with a subset of plugins\\nBy default, LLM will load all plugins that are installed in the same virtual environment as LLM itself.\\nYou can control the set of plugins that is loaded using the LLM_LOAD_PLUGINS environment variable.\\nSet that to the empty string to disable all plugins:\\nLLM_LOAD_PLUGINS= \\'\\'llm ...\\nOr to a comma-separated list of plugin names to load only those plugins:\\nLLM_LOAD_PLUGINS= \\'llm-gpt4all,llm-cluster \\'llm ...\\nYou can use the llm plugins command to check that it is working correctly:\\n34 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 40}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nLLM_LOAD_PLUGINS= ''llm plugins\\n2.6.2 Plugin directory\\nThe following plugins are available for LLM. Here’s how to install them .\\nLocal models\\nThese plugins all help you run LLMs directly on your own computer:\\n•llm-llama-cpp uses llama.cpp to run models published in the GGUF format.\\n•llm-mlccanrunlocalmodelsreleasedbytheMLCproject,includingmodelsthatcantakeadvantageoftheGPU\\non Apple Silicon M1/M2 devices.\\n•llm-gpt4all adds support for various models released by the GPT4All project that are optimized to run locally\\non your own machine. These models include versions of Vicuna, Orca, Falcon and MPT - here’s a full list of\\nmodels.\\n•llm-mpt30b adds support for the MPT-30B local model.\\n•llm-ollama adds support for local models run using Ollama.\\n•llm-llamafile adds support for local models that are running locally using llamafile.\\nRemote APIs\\nThese plugins can be used to interact with remotely hosted models via their API:\\n•llm-mistral adds support for Mistral AI’s language and embedding models.\\n•llm-gemini adds support for Google’s Gemini models.\\n•llm-claude by Tom Viner adds support for Claude 2.1 and Claude Instant 2.1 by Anthropic.\\n•llm-claude-3 supports Anthropic’s Claude 3 family of models.\\n•llm-command-r supports Cohere’s Command R and Command R Plus API models.\\n•llm-reka supports the Reka family of models via their API.\\n•llm-perplexity by Alexandru Geana supports the Perplexity Labs API models, including\\nllama-3-sonar-large-32k-online which can search for things online and llama-3-70b-instruct .\\n•llm-groq by Moritz Angermann provides access to fast models hosted by Groq.\\n•llm-anyscale-endpoints supports models hosted on the Anyscale Endpoints platform, including Llama 2 70B.\\n•llm-replicate adds support for remote models hosted on Replicate, including Llama 2 from Meta AI.\\n•llm-fireworks supports models hosted by Fireworks AI.\\n•llm-palm adds support for Google’s PaLM 2 model.\\n•llm-openrouter provides access to models hosted on OpenRouter.\\n•llm-cohere by Alistair Shepherd provides cohere-generate andcohere-summarize API models, powered\\nby Cohere.\\n•llm-bedrock-anthropic by Sean Blakey adds support for Claude and Claude Instant by Anthropic via Amazon\\nBedrock.\\n2.6. Plugins 35\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 41}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n•llm-bedrock-meta by Fabian Labat adds support for Llama 2 and Llama 3 by Meta via Amazon Bedrock.\\n•llm-together adds support for the Together AI extensive family of hosted openly licensed models.\\nIfanAPImodelhostprovidesanOpenAI-compatibleAPIyoucanalsoconfigureLLMtotalktoitwithoutneedingan\\nextra plugin.\\nEmbedding models\\nEmbedding models are models that can be used to generate and store embedding vectors for text.\\n•llm-sentence-transformers adds support for embeddings using the sentence-transformers library, which pro-\\nvides access to a wide range of embedding models.\\n•llm-clipprovidestheCLIPmodel,whichcanbeusedtoembedimagesandtextinthesamevectorspace,enabling\\ntext search against images. See Build an image search engine with llm-clip for more on this plugin.\\n•llm-embed-jina provides Jina AI’s 8K text embedding models.\\n•llm-embed-onnx provides seven embedding models that can be executed using the ONNX model framework.\\nExtra commands\\n•llm-cmd accepts a prompt for a shell command, runs that prompt and populates the result in your shell so you\\ncan review it, edit it and then hit <enter> to execute or ctrl+cto cancel.\\n•llm-python adds allm python command for running a Python interpreter in the same virtual environment as\\nLLM. This is useful for debugging, and also provides a convenient way to interact with the LLM Python API if\\nyou installed LLM using Homebrew or pipx.\\n•llm-cluster adds allm cluster command for calculating clusters for a collection of embeddings. Calculated\\nclusters can then be passed to a Large Language Model to generate a summary description.\\nJust for fun\\n•llm-markov addsasimplemodelthatgeneratesoutputusingaMarkovchain. Thisexampleisusedinthetutorial\\nWriting a plugin to support a new model.\\n2.6.3 Plugin hooks\\nPlugins use plugin hooks to customize LLM’s behavior. These hooks are powered by the Pluggy plugin system.\\nEachplugincanimplementoneormorehooksusingthe@hookimpldecoratoragainstoneofthehookfunctionnames\\ndescribed on this page.\\nLLM imitates the Datasette plugin system. The Datasette plugin documentation describes how plugins work.\\n36 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 42}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nregister_commands(cli)\\nThis hook adds new commands to the llmCLI tool - for example llm extra-command .\\nThis example plugin adds a new hello-world command that prints “Hello world!”:\\nfrom llm import hookimpl\\nimport click\\n@hookimpl\\ndefregister_commands(cli):\\n@cli.command(name=\"hello-world\")\\ndefhello_world():\\n\"Print hello world\"\\nclick.echo(\"Hello world!\")\\nThis new command will be added to llm --help and can be run using llm hello-world .\\nregister_models(register)\\nThis hook can be used to register one or more additional models.\\nimport llm\\n@llm.hookimpl\\ndefregister_models(register):\\nregister(HelloWorld())\\nclass HelloWorld (llm.Model):\\nmodel_id = \"helloworld\"\\ndefexecute(self, prompt, stream, response):\\nreturn[\"hello world\"]\\nWriting a plugin to support a new model describes how to use this hook in detail.\\n2.6.4 Writing a plugin to support a new model\\nThis tutorial will walk you through developing a new plugin for LLM that adds support for a new Large Language\\nModel.\\nWe will be developing a plugin that implements a simple Markov chain to generate words based on an input string.\\nMarkovchainsarenottechnicallylargelanguagemodels,buttheyprovideausefulexercisefordemonstratinghowthe\\nLLM tool can be extended through plugins.\\n2.6. Plugins 37'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 43}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nThe initial structure of the plugin\\nFirst create a new directory with the name of your plugin - it should be called something like llm-markov .\\nmkdir llm-markov\\ncd llm-markov\\nIn that directory create a file called llm_markov.py containing this:\\nimport llm\\n@llm.hookimpl\\ndefregister_models(register):\\nregister(Markov())\\nclass Markov (llm.Model):\\nmodel_id = \"markov\"\\ndefexecute(self, prompt, stream, response, conversation):\\nreturn[\"hello world\"]\\nThedef register_models() function here is called by the plugin system (thanks to the @hookimpl decorator). It\\nuses theregister() function passed to it to register an instance of the new model.\\nTheMarkovclass implements the model. It sets a model_id - an identifier that can be passed to llm -min order to\\nidentify the model to be executed.\\nThelogicforexecutingthemodelgoesinthe execute() method. We’llextendthistodosomethingmoreusefulina\\nlater step.\\nNext, create a pyproject.toml file. This is necessary to tell LLM how to load your plugin:\\n[project]\\nname = \"llm-markov\"\\nversion = \"0.1\"\\n[project.entry-points.llm]\\nmarkov = \"llm_markov\"\\nThisisthesimplestpossibleconfiguration. Itdefinesapluginnameandprovidesanentrypointfor llmtellingithow\\nto load the plugin.\\nIf you are comfortable with Python virtual environments you can create one now for your project, activate it and run\\npip install llm before the next step.\\nIf you aren’t familiar with virtual environments, don’t worry: you can develop plugins without them. You’ll need to\\nhave LLM installed using Homebrew or pipxor one of the other installation options.\\n38 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 44}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nInstalling your plugin to try it out\\nHaving created a directory with a pyproject.toml file and an llm_markov.py file, you can install your plugin into\\nLLM by running this from inside your llm-markov directory:\\nllm install -e .\\nThe-estands for “editable” - it means you’ll be able to make further changes to the llm_markov.py file that will be\\nreflected without you having to reinstall the plugin.\\nThe.means the current directory. You can also install editable plugins by passing a path to their directory this:\\nllm install -e path/to/llm-markov\\nTo confirm that your plugin has installed correctly, run this command:\\nllm plugins\\nThe output should look like this:\\n[\\n{\\n\"name\": \"llm-markov\",\\n\"hooks\": [\\n\"register_models\"\\n],\\n\"version\" : \"0.1\"\\n},\\n{\\n\"name\": \"llm.default_plugins.openai_models\",\\n\"hooks\": [\\n\"register_commands\",\\n\"register_models\"\\n]\\n}\\n]\\nThis command lists default plugins that are included with LLM as well as new plugins that have been installed.\\nNow let’s try the plugin by running a prompt through it:\\nllm -m markov \"the cat sat on the mat\"\\nIt outputs:\\nhello world\\nNext, we’ll make it execute and return the results of a Markov chain.\\n2.6. Plugins 39'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 45}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nBuilding the Markov chain\\nMarkov chains can be thought of as the simplest possible example of a generative language model. They work by\\nbuilding an index of words that have been seen following other words.\\nHere’s what that index looks like for the phrase “the cat sat on the mat”\\n{\\n\"the\": [\"cat\", \"mat\"],\\n\"cat\": [\"sat\"],\\n\"sat\": [\"on\"],\\n\"on\": [\"the\"]\\n}\\nHere’s a Python function that builds that data structure from a text input:\\ndefbuild_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nforiinrange(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturntransitions\\nWe can try that out by pasting it into the interactive Python interpreter and running this:\\n>>>transitions = build_markov_table(\"the cat sat on the mat\")\\n>>>transitions\\n{\\'the\\': [\\'cat\\',\\'mat\\'],\\'cat\\': [\\'sat\\'],\\'sat\\': [\\'on\\'],\\'on\\': [\\'the\\']}\\nExecuting the Markov chain\\nTo execute the model, we start with a word. We look at the options for words that might come next and pick one of\\nthose at random. Then we repeat that process until we have produced the desired number of output words.\\nSomewordsmightnothaveanyfollowingwordsfromourtrainingsentence. Forourimplementationwewillfallback\\non picking a random word from our collection.\\nWe will implement this as a Python generator, using the yield keyword to produce each token:\\ndefgenerate(transitions, length, start_word= None):\\nall_words = list(transitions.keys())\\nnext_word = start_word orrandom.choice(all_words)\\nforiinrange(length):\\nyieldnext_word\\noptions = transitions.get(next_word) orall_words\\nnext_word = random.choice(options)\\nIf you aren’t familiar with generators, the above code could also be implemented like this - creating a Python list and\\nreturning it at the end of the function:\\n40 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 46}, page_content='LLM documentation, Release 0.16-1-gd654c95\\ndefgenerate_list(transitions, length, start_word= None):\\nall_words = list(transitions.keys())\\nnext_word = start_word orrandom.choice(all_words)\\noutput = []\\nforiinrange(length):\\noutput.append(next_word)\\noptions = transitions.get(next_word) orall_words\\nnext_word = random.choice(options)\\nreturnoutput\\nYou can try out the generate() function like this:\\nlookup = build_markov_table(\"the cat sat on the mat\")\\nforword ingenerate(transitions, 20):\\nprint(word)\\nOr you can generate a full string sentence with it like this:\\nsentence = \" \".join(generate(transitions, 20))\\nAdding that to the plugin\\nOurexecute() method from earlier currently returns the list [\"hello world\"] .\\nUpdate that to use our new Markov chain generator instead. Here’s the full text of the new llm_markov.py file:\\nimport llm\\nimport random\\n@llm.hookimpl\\ndefregister_models(register):\\nregister(Markov())\\ndefbuild_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nforiinrange(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturntransitions\\ndefgenerate(transitions, length, start_word= None):\\nall_words = list(transitions.keys())\\nnext_word = start_word orrandom.choice(all_words)\\nforiinrange(length):\\nyieldnext_word\\noptions = transitions.get(next_word) orall_words\\nnext_word = random.choice(options)\\nclass Markov (llm.Model):\\nmodel_id = \"markov\"\\n(continues on next page)\\n2.6. Plugins 41'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 47}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\ndefexecute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nforword ingenerate(transitions, 20):\\nyieldword + \\' \\'\\nTheexecute() methodcanaccessthetextpromptthattheuserprovidedusing prompt.prompt -promptisaPrompt\\nobject that might include other more advanced input details as well.\\nNow when you run this you should see the output of the Markov chain!\\nllm -m markov \"the cat sat on the mat\"\\nthe mat the cat sat on the cat sat on the mat cat sat on the mat cat sat on\\nUnderstanding execute()\\nThe full signature of the execute() method is:\\ndefexecute(self, prompt, stream, response, conversation):\\nThepromptargument is a Promptobject that contains the text that the user provided, the system prompt and the\\nprovided options.\\nstreamis a boolean that says if the model is being run in streaming mode.\\nresponse is theResponse object that is being created by the model. This is provided so you can write additional\\ninformation to response.response_json , which may be logged to the database.\\nconversation is theConversation that the prompt is a part of - or Noneif no conversation was provided. Some\\nmodels may use conversation.responses to access previous prompts and responses in the conversation and use\\nthem to construct a call to the LLM that includes previous context.\\nPrompts and responses are logged to the database\\nThe prompt and the response will be logged to a SQLite database automatically by LLM. You can see the single most\\nrecent addition to the logs using:\\nllm logs -n 1\\nThe output should look something like this:\\n[\\n{\\n\"id\": \"01h52s4yez2bd1qk2deq49wk8h\",\\n\"model\": \"markov\",\\n\"prompt\" : \"the cat sat on the mat\",\\n\"system\" :null,\\n\"prompt_json\" :null,\\n\"options_json\" : {},\\n\"response\" : \"on the cat sat on the cat sat on the mat cat sat on the cat sat on the␣\\n˓→cat \",\\n(continues on next page)\\n42 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 48}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n\"response_json\" :null,\\n\"conversation_id\" : \"01h52s4yey7zc5rjmczy3ft75g\",\\n\"duration_ms\" : 0,\\n\"datetime_utc\" : \"2023-07-11T15:29:34.685868\",\\n\"conversation_name\" : \"the cat sat on the mat\",\\n\"conversation_model\" : \"markov\"\\n}\\n]\\nPlugins can log additional information to the database by assigning a dictionary to the response.response_json\\nproperty during the execute() method.\\nHere’s how to include that full transitions table in the response_json in the log:\\ndefexecute(self, prompt, stream, response, conversation):\\ntext = self.prompt.prompt\\ntransitions = build_markov_table(text)\\nforword ingenerate(transitions, 20):\\nyieldword + \\' \\'\\nresponse.response_json = {\"transitions\": transitions}\\nNow when you run the logs command you’ll see that too:\\nllm logs -n 1\\n[\\n{\\n\"id\": 623,\\n\"model\": \"markov\",\\n\"prompt\" : \"the cat sat on the mat\",\\n\"system\" :null,\\n\"prompt_json\" :null,\\n\"options_json\" : {},\\n\"response\" : \"on the mat the cat sat on the cat sat on the mat sat on the cat sat on␣\\n˓→the \",\\n\"response_json\" : {\\n\"transitions\" : {\\n\"the\": [\\n\"cat\",\\n\"mat\"\\n],\\n\"cat\": [\\n\"sat\"\\n],\\n\"sat\": [\\n\"on\"\\n],\\n\"on\": [\\n\"the\"\\n]\\n}\\n},\\n\"reply_to_id\" :null,\\n(continues on next page)\\n2.6. Plugins 43'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 49}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n\"chat_id\" :null,\\n\"duration_ms\" : 0,\\n\"datetime_utc\" : \"2023-07-06T01:34:45.376637\"\\n}\\n]\\nIn this particular case this isn’t a great idea here though: the transitions table is duplicate information, since it can\\nbe reproduced from the input data - and it can get really large for longer prompts.\\nAdding options\\nLLM models can take options. For large language models these can be things like temperature ortop_k.\\nOptions are passed using the -o/--option command line parameters, for example:\\nllm -m gpt4 \"ten pet pelican names\" -o temperature 1.5\\nWe’re going to add two options to our Markov chain model:\\n•length: Number of words to generate\\n•delay: a floating point number of Delay in between output token\\nThedelaytokenwillletussimulateastreaminglanguagemodel,wheretokenstaketimetogenerateandarereturned\\nby theexecute() function as they become ready.\\nOptions are defined using an inner class on the model, called Options. It should extend the llm.Options class.\\nFirst, add this import to the top of your llm_markov.py file:\\nfrom typing import Optional\\nThen add this Options class to your model:\\nclass Markov (Model):\\nmodel_id = \"markov\"\\nclass Options (llm.Options):\\nlength: Optional[int] = None\\ndelay: Optional[float] = None\\nLet’s add extra validation rules to our options. Length must be at least 2. Duration must be between 0 and 10.\\nTheOptions class uses Pydantic 2, which can support all sorts of advanced validation rules.\\nWe can also add inline documentation, which can then be displayed by the llm models --options command.\\nAdd these imports to the top of llm_markov.py :\\nfrom pydantic import field_validator, Field\\nWe can now add Pydantic field validators for our two new rules, plus inline documentation:\\nclass Options (llm.Options):\\nlength: Optional[int] = Field(\\ndescription=\"Number of words to generate\",\\ndefault= None\\n(continues on next page)\\n44 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 50}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n)\\ndelay: Optional[float] = Field(\\ndescription=\"Seconds to delay between each token\",\\ndefault= None\\n)\\n@field_validator (\"length\")\\ndefvalidate_length(cls, length):\\niflength is None:\\nreturn None\\niflength < 2:\\nraiseValueError(\"length must be >= 2\")\\nreturnlength\\n@field_validator (\"delay\")\\ndefvalidate_delay(cls, delay):\\nifdelay is None:\\nreturn None\\nif not0 <= delay <= 10:\\nraiseValueError(\"delay must be between 0 and 10\")\\nreturndelay\\nLets test our options validation:\\nllm -m markov \"the cat sat on the mat\" -o length -1\\nError: length\\nValue error, length must be >= 2\\nNext,wewillmodifyour execute() methodtohandlethoseoptions. Addthistothebeginningof llm_markov.py :\\nimport time\\nThen replace the execute() method with this one:\\ndefexecute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nlength = prompt.options.length or20\\nforword ingenerate(transitions, length):\\nyieldword + \\' \\'\\nifprompt.options.delay:\\ntime.sleep(prompt.options.delay)\\nAddcan_stream = True to the top of the Markovmodel class, on the line below `model_id = “markov”. This tells\\nLLM that the model is able to stream content to the console.\\nThe fullllm_markov.py file should now look like this:\\nimport llm\\nimport random\\nimport time\\nfrom typing import Optional\\n(continues on next page)\\n2.6. Plugins 45'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 51}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nfrom pydantic import field_validator, Field\\n@llm.hookimpl\\ndefregister_models(register):\\nregister(Markov())\\ndefbuild_markov_table(text):\\nwords = text.split()\\ntransitions = {}\\n# Loop through all but the last word\\nforiinrange(len(words) - 1):\\nword = words[i]\\nnext_word = words[i + 1]\\ntransitions.setdefault(word, []).append(next_word)\\nreturntransitions\\ndefgenerate(transitions, length, start_word= None):\\nall_words = list(transitions.keys())\\nnext_word = start_word orrandom.choice(all_words)\\nforiinrange(length):\\nyieldnext_word\\noptions = transitions.get(next_word) orall_words\\nnext_word = random.choice(options)\\nclass Markov (llm.Model):\\nmodel_id = \"markov\"\\ncan_stream = True\\nclass Options (llm.Options):\\nlength: Optional[int] = Field(\\ndescription=\"Number of words to generate\", default= None\\n)\\ndelay: Optional[float] = Field(\\ndescription=\"Seconds to delay between each token\", default= None\\n)\\n@field_validator (\"length\")\\ndefvalidate_length(cls, length):\\niflength is None:\\nreturn None\\niflength < 2:\\nraiseValueError(\"length must be >= 2\")\\nreturnlength\\n@field_validator (\"delay\")\\ndefvalidate_delay(cls, delay):\\nifdelay is None:\\nreturn None\\n(continues on next page)\\n46 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 52}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nif not0 <= delay <= 10:\\nraiseValueError(\"delay must be between 0 and 10\")\\nreturndelay\\ndefexecute(self, prompt, stream, response, conversation):\\ntext = prompt.prompt\\ntransitions = build_markov_table(text)\\nlength = prompt.options.length or20\\nforword ingenerate(transitions, length):\\nyieldword + \" \"\\nifprompt.options.delay:\\ntime.sleep(prompt.options.delay)\\nNow we can request a 20 word completion with a 0.1s delay between tokens like this:\\nllm -m markov \"the cat sat on the mat\" \\\\\\n-o length 20 -o delay 0.1\\nLLMprovidesa --no-stream optionuserscanusetoturnoffstreaming. UsingthatoptioncausesLLMtogatherthe\\nresponse from the stream and then return it to the console in one block. You can try that like this:\\nllm -m markov \"the cat sat on the mat\" \\\\\\n-o length 20 -o delay 0.1 --no-stream\\nIn this case it will still delay for 2s total while it gathers the tokens, then output them all at once.\\nThat--no-stream option causes the streamargument passed to execute() to be false. Your execute() method\\ncan then behave differently depending on whether it is streaming or not.\\nOptions are also logged to the database. You can see those here:\\nllm logs -n 1\\n[\\n{\\n\"id\": 636,\\n\"model\": \"markov\",\\n\"prompt\" : \"the cat sat on the mat\",\\n\"system\" :null,\\n\"prompt_json\" :null,\\n\"options_json\" : {\\n\"length\" : 20,\\n\"delay\": 0.1\\n},\\n\"response\" : \"the mat on the mat on the cat sat on the mat sat on the mat cat sat on␣\\n˓→the \",\\n\"response_json\" :null,\\n\"reply_to_id\" :null,\\n\"chat_id\" :null,\\n\"duration_ms\" : 2063,\\n\"datetime_utc\" : \"2023-07-07T03:02:28.232970\"\\n}\\n]\\n2.6. Plugins 47'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 53}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nDistributing your plugin\\nThere are many different options for distributing your new plugin so other people can try it out.\\nYoucancreateadownloadablewheelor .zipor.tar.gz files,orsharethepluginthroughGitHubGistsorreposito-\\nries.\\nYou can also publish your plugin to PyPI, the Python Package Index.\\nWheels and sdist packages\\nTheeasiestoptionistoproduceadistributablepackageistousethe buildcommand. First,installthe buildpackage\\nby running this:\\npython -m pip install build\\nThen runbuildin your plugin directory to create the packages:\\npython -m build\\nThis will create two files: dist/llm-markov-0.1.tar.gz anddist/llm-markov-0.1-py3-none-any.whl .\\nEither of these files can be used to install the plugin:\\nllm install dist/llm_markov-0.1-py3-none-any.whl\\nIf you host this file somewhere online other people will be able to install it using pip install against the URL to\\nyour package:\\nllm install 'https://.../llm_markov-0.1-py3-none-any.whl '\\nYou can run the following command at any time to uninstall your plugin, which is useful for testing out different\\ninstallation methods:\\nllm uninstall llm-markov -y\\nGitHub Gists\\nA neat quick option for distributing a simple plugin is to host it in a GitHub Gist. These are available for free with a\\nGitHub account, and can be public or private. Gists can contain multiple files but don’t support directory structures -\\nwhich is OK, because our plugin is just two files, pyproject.toml andllm_markov.py .\\nHere’s an example Gist I created for this tutorial:\\nhttps://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d\\nYoucanturnaGistintoaninstallable .zipURLbyright-clickingonthe“DownloadZIP”buttonandselecting“Copy\\nLink”. Here’s that link for my example Gist:\\nhttps://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d/archive/\\ncc50c854414cb4deab3e3ab17e7e1e07d45cba0c.zip\\nThe plugin can be installed using the llm install command like this:\\nllm install 'https://gist.github.com/simonw/6e56d48dc2599bffba963cef0db27b6d/archive/\\n˓→cc50c854414cb4deab3e3ab17e7e1e07d45cba0c.zip '\\n48 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 54}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nGitHub repositories\\nThesametrickworksforregularGitHubrepositoriesaswell: the“DownloadZIP”buttoncanbefoundbyclickingthe\\ngreen “Code” button at the top of the repository. The URL which that provide scan then be used to install the plugin\\nthat lives in that repository.\\nPublishing plugins to PyPI\\nThe Python Package Index (PyPI) is the official repository for Python packages. You can upload your plugin to PyPI\\nand reserve a name for it - once you have done that, anyone will be able to install your plugin using llm install\\n<name>.\\nFollow these instructions to publish a package to PyPI. The short version:\\npython -m pip install twine\\npython -m twine upload dist/*\\nYou will need an account on PyPI, then you can enter your username and password - or create a token in the PyPI\\nsettings and use __token__ as the username and the token as the password.\\nAdding metadata\\nBeforeuploadingapackagetoPyPIit’sagoodideatoadddocumentationandexpand pyproject.toml withadditional\\nmetadata.\\nCreate aREADME.md file in the root of your plugin directory with instructions about how to install, configure and use\\nyour plugin.\\nYou can then replace pyproject.toml with something like this:\\n[project]\\nname = \"llm-markov\"\\nversion = \"0.1\"\\ndescription = \"Plugin for LLM adding a Markov chain generating model\"\\nreadme = \"README.md\"\\nauthors = [{name = \"Simon Willison\"}]\\nlicense = {text = \"Apache-2.0\"}\\nclassifiers = [\\n\"License :: OSI Approved :: Apache Software License\"\\n]\\ndependencies = [\\n\"llm\"\\n]\\nrequires-python = \">3.7\"\\n[project.urls]\\nHomepage = \"https://github.com/simonw/llm-markov\"\\nChangelog = \"https://github.com/simonw/llm-markov/releases\"\\nIssues = \"https://github.com/simonw/llm-markov/issues\"\\n[project.entry-points.llm]\\nmarkov = \"llm_markov\"\\nThis will pull in your README to be displayed as part of your project’s listing page on PyPI.\\n2.6. Plugins 49'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 55}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nIt addsllmas a dependency, ensuring it will be installed if someone tries to install your plugin package without it.\\nIt adds some links to useful pages (you can drop the project.urls section if those links are not useful for your\\nproject).\\nYoushoulddropa LICENSE fileintotheGitHubrepositoryforyourpackageaswell. IliketousetheApache2license\\nlike this.\\nWhat to do if it breaks\\nSometimes you may make a change to your plugin that causes it to break, preventing llmfrom starting. For example\\nyou may see an error like this one:\\n$ llm \\'hi\\'\\nTraceback (most recent call last):\\n...\\nFile llm-markov/llm_markov.py\", line 10\\nregister(Markov()):\\n^\\nSyntaxError: invalid syntax\\nYou may find that you are unable to uninstall the plugin using llm uninstall llm-markov because the command\\nitself fails with the same error.\\nShould this happen, you can uninstall the plugin after first disabling it using the LLM_LOAD_PLUGINS environment\\nvariable like this:\\nLLM_LOAD_PLUGINS= \\'\\'llm uninstall llm-markov\\n2.6.5 Utility functions for plugins\\nLLM provides some utility functions that may be useful to plugins.\\nllm.user_dir()\\nLLM stores various pieces of logging and configuration data in a directory on the user’s machine.\\nOn macOS this directory is ~/Library/Application Support/io.datasette.llm , but this will differ on other\\noperating systems.\\nThellm.user_dir() functionreturnsthepathtothisdirectoryasa pathlib.Path object,aftercreatingthatdirec-\\ntory if it does not yet exist.\\nPlugins can use this to store their own data in a subdirectory of this directory.\\nimport llm\\nuser_dir = llm.user_dir()\\nplugin_dir = data_path = user_dir / \"my-plugin\"\\nplugin_dir.mkdir(exist_ok= True)\\ndata_path = plugin_dir / \"plugin-data.db\"\\n50 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 56}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm.ModelError\\nIf your model encounters an error that should be reported to the user you can raise this exception. For example:\\nimport llm\\nraiseModelError(\"MPT model not installed - try running \\'llm mpt30b download \\'\")\\nThis will be caught by the CLI layer and displayed to the user as an error message.\\nResponse.fake()\\nWhen writing tests for a model it can be useful to generate fake response objects, for example in this test from llm-\\nmpt30b:\\ndeftest_build_prompt_conversation():\\nmodel = llm.get_model(\"mpt\")\\nconversation = model.conversation()\\nconversation.responses = [\\nllm.Response.fake(model, \"prompt 1\", \"system 1\", \"response 1\"),\\nllm.Response.fake(model, \"prompt 2\", None, \"response 2\"),\\nllm.Response.fake(model, \"prompt 3\", None, \"response 3\"),\\n]\\nlines = model.build_prompt(llm.Prompt(\"prompt 4\", model), conversation)\\nassertlines == [\\n\"<|im_start|>system\\\\system 1<|im_end|> \\\\n\",\\n\"<|im_start|>user \\\\nprompt 1<|im_end|> \\\\n\",\\n\"<|im_start|>assistant \\\\nresponse 1<|im_end|> \\\\n\",\\n\"<|im_start|>user \\\\nprompt 2<|im_end|> \\\\n\",\\n\"<|im_start|>assistant \\\\nresponse 2<|im_end|> \\\\n\",\\n\"<|im_start|>user \\\\nprompt 3<|im_end|> \\\\n\",\\n\"<|im_start|>assistant \\\\nresponse 3<|im_end|> \\\\n\",\\n\"<|im_start|>user \\\\nprompt 4<|im_end|> \\\\n\",\\n\"<|im_start|>assistant \\\\n\",\\n]\\nThe signature of llm.Response.fake() is:\\ndeffake(cls, model: Model, prompt: str, system: str, response: str):\\n2.7 Model aliases\\nLLM supports model aliases, which allow you to refer to a model by a short name instead of its full ID.\\n2.7. Model aliases 51'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 57}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.7.1 Listing aliases\\nTo list current aliases, run this:\\nllm aliases\\nExample output:\\n3.5 : gpt-3.5-turbo\\nchatgpt : gpt-3.5-turbo\\nchatgpt-16k : gpt-3.5-turbo-16k\\n3.5-16k : gpt-3.5-turbo-16k\\n4 : gpt-4\\ngpt4 : gpt-4\\n4-32k : gpt-4-32k\\ngpt-4-turbo-preview : gpt-4-turbo\\n4-turbo : gpt-4-turbo\\n4t : gpt-4-turbo\\n4o : gpt-4o\\n4o-mini : gpt-4o-mini\\n3.5-instruct : gpt-3.5-turbo-instruct\\nchatgpt-instruct : gpt-3.5-turbo-instruct\\nada : ada-002 (embedding)\\nAdd--jsonto get that list back as JSON:\\nllm aliases list --json\\nExample output:\\n{\\n\"3.5\": \"gpt-3.5-turbo\",\\n\"chatgpt\" : \"gpt-3.5-turbo\",\\n\"chatgpt-16k\" : \"gpt-3.5-turbo-16k\",\\n\"3.5-16k\" : \"gpt-3.5-turbo-16k\",\\n\"4\": \"gpt-4\",\\n\"gpt4\": \"gpt-4\",\\n\"4-32k\": \"gpt-4-32k\",\\n\"ada\": \"ada-002\"\\n}\\n2.7.2 Adding a new alias\\nThellm aliases set <alias> <model-id> command can be used to add a new alias:\\nllm aliases set turbo gpt-3.5-turbo-16k\\nNow you can run the gpt-3.5-turbo-16k model using the turboalias like this:\\nllm -m turbo \\'An epic Greek-style saga about a cheesecake that builds a SQL database␣\\n˓→from scratch \\'\\nAliasescanbesetforbothregularmodelsand embeddingmodels usingthesamecommand. Tosetanaliasof oaifor\\nthe OpenAI ada-002 embedding model use this:\\n52 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 58}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm aliases set oai ada-002\\nNow you can embed a string using that model like so:\\nllm embed -c \\'hello world \\'-m oai\\nOutput:\\n[-0.014945968054234982, 0.0014304015785455704, ...]\\n2.7.3 Removing an alias\\nThellm aliases remove <alias> command will remove the specified alias:\\nllm aliases remove turbo\\n2.7.4 Viewing the aliases file\\nAliases are stored in an aliases.json file in the LLM configuration directory.\\nTo see the path to that file, run this:\\nllm aliases path\\nTo view the content of that file, run this:\\ncat \" $(llm aliases path )\"\\n2.8 Python API\\nLLM provides a Python API for executing prompts, in addition to the command-line interface.\\nUnderstanding this API is also important for writing Plugins.\\n2.8.1 Basic prompt execution\\nTo run a prompt against the gpt-4o-mini model, run this:\\nimport llm\\nmodel = llm.get_model(\"gpt-4o-mini\")\\n# Optional, you can configure the key in other ways:\\nmodel.key = \"sk-...\"\\nresponse = model.prompt(\"Five surprising names for a pet pelican\")\\nprint(response.text())\\nThellm.get_model() functionacceptsmodelnamesoraliases. Youcanalsoomitittousethecurrentlyconfigured\\ndefault model, which is gpt-4o-mini if you have not changed the default.\\n2.8. Python API 53'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 59}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nIn this example the key is set by Python code. You can also provide the key using the OPENAI_API_KEY environment\\nvariable, or use the llm keys set openai command to store it in a keys.json file, seeAPI key management .\\nThe__str__() method of response also returns the text of the response, so you can do this instead:\\nprint(llm.get_model().prompt(\"Five surprising names for a pet pelican\"))\\nYou can run this command to see a list of available models and their aliases:\\nllm models\\nIf you have set a OPENAI_API_KEY environment variable you can omit the model.key = line.\\nCallingllm.get_model() with an invalid model name will raise a llm.UnknownModelError exception.\\nSystem prompts\\nFor models that accept a system prompt, pass it as system=\"...\" :\\nresponse = model.prompt(\\n\"Five surprising names for a pet pelican\",\\nsystem=\"Answer like GlaDOS\"\\n)\\nModel options\\nFor models that support options (view those with llm models --options ) you can pass options as keyword argu-\\nments to the .prompt() method:\\nmodel = llm.get_model()\\nprint(model.prompt(\"Names for otters\", temperature=0.2))\\nModels from plugins\\nAnymodelsyouhaveinstalledaspluginswillalsobeavailablethroughthismechanism,forexampletouseAnthropic’s\\nClaude 3.5 Sonnet model with llm-claude-3:\\npip install llm-claude-3\\nThen in your Python code:\\nimport llm\\nmodel = llm.get_model(\"claude-3.5-sonnet\")\\n# Use this if you have not set the key using \\'llm keys set claude \\':\\nmodel.key = \\'YOUR_API_KEY_HERE \\'\\nresponse = model.prompt(\"Five surprising names for a pet pelican\")\\nprint(response.text())\\nSome models do not use API keys at all.\\n54 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 60}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.8.2 Streaming responses\\nFor models that support it you can stream responses as they are generated, like this:\\nresponse = model.prompt(\"Five diabolical names for a pet goat\")\\nforchunk inresponse:\\nprint(chunk, end=\"\")\\nTheresponse.text() methoddescribedearlierdoesthisforyou-itrunsthroughtheiteratorandgatherstheresults\\ninto a string.\\nIf a response has been evaluated, response.text() will continue to return the same string.\\n2.8.3 Conversations\\nLLM supports conversations , where you ask follow-up questions of a model as part of an ongoing conversation.\\nTo start a new conversation, use the model.conversation() method:\\nmodel = llm.get_model()\\nconversation = model.conversation()\\nYou can then use the conversation.prompt() method to execute prompts against this conversation:\\nresponse = conversation.prompt(\"Five fun facts about pelicans\")\\nprint(response.text())\\nThis works exactly the same as the model.prompt() method, except that the conversation will be maintained across\\nmultiple prompts. So if you run this next:\\nresponse2 = conversation.prompt(\"Now do skunks\")\\nprint(response2.text())\\nYou will get back five fun facts about skunks.\\nAccessconversation.responses for a list of all of the responses that have so far been returned during the conver-\\nsation.\\n2.8.4 Other functions\\nThellmtop level package includes some useful utility functions.\\nset_alias(alias, model_id)\\nThellm.set_alias() function can be used to define a new alias:\\nimport llm\\nllm.set_alias(\"mini\", \"gpt-4o-mini\")\\nThe second argument can be a model identifier or another alias, in which case that alias will be resolved.\\nIf thealiases.json file does not exist or contains invalid JSON it will be created or overwritten.\\n2.8. Python API 55'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 61}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nremove_alias(alias)\\nRemoves the alias with the given name from the aliases.json file.\\nRaisesKeyError if the alias does not exist.\\nimport llm\\nllm.remove_alias(\"turbo\")\\nset_default_model(alias)\\nThis sets the default model to the given model ID or alias. Any changes to defaults will be persisted in the LLM\\nconfiguration folder, and will affect all programs using LLM on the system, including the llmCLI tool.\\nimport llm\\nllm.set_default_model(\"claude-3.5-sonnet\")\\nget_default_model()\\nThis returns the currently configured default model, or gpt-4o-mini if no default has been set.\\nimport llm\\nmodel_id = llm.get_default_model()\\nTo detect if no default has been set you can use this pattern:\\nifllm.get_default_model(default= None)is None:\\nprint(\"No default has been set\")\\nHere thedefault= parameter specifies the value that should be returned if there is no configured default.\\nset_default_embedding_model(alias) and get_default_embedding_model()\\nThese two methods work the same as set_default_model() andget_default_model() but for the default em-\\nbedding model instead.\\n2.9 Prompt templates\\nPrompt templates can be created to reuse useful prompts with different input data.\\n56 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 62}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n2.9.1 Getting started\\nThe easiest way to create a template is using the --save template_name option.\\nHere’s how to create a template for summarizing text:\\nllm 'Summarize this: $input '--save summarize\\nYou can also create templates using system prompts:\\nllm --system 'Summarize this '--save summarize\\nYou can set the default model for a template using --model:\\nllm --system 'Summarize this '--model gpt-4 --save summarize\\nYou can also save default parameters:\\nllm --system 'Summarize this text in the voice of $voice '\\\\\\n--model gpt-4 -p voice GlaDOS --save summarize\\n2.9.2 Using a template\\nYou can execute a named template using the -t/--template option:\\ncurl -s https://example.com/ | llm -t summarize\\nThis can be combined with the -moption to specify a different model:\\ncurl -s https://llm.datasette.io/en/latest/ | \\\\\\nllm -t summarize -m gpt-3.5-turbo-16k\\n2.9.3 Listing available templates\\nThis command lists all available templates:\\nllm templates\\nThe output looks something like this:\\ncmd : system: reply with macos terminal commands only, no extra information\\nglados : system: You are GlaDOS prompt: Summarize this: $input\\n2.9. Prompt templates 57\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 63}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.9.4 Templates as YAML files\\nTemplates are stored as YAML files on disk.\\nYou can edit (or create) a YAML file for a template using the llm templates edit command:\\nllm templates edit summarize\\nThis will open the system default editor.\\nTip:Youcancontrolwhicheditorwillbeusedhereusingthe EDITORenvironmentvariable-forexample,touseVS\\nCode:\\nexport EDITOR=\"code -w\"\\nAdd that to your ~/.zshrc or~/.bashrc file depending on which shell you use ( zshis the default on macOS since\\nmacOS Catalina in 2019).\\nYou can also create a file called summary.yaml in the folder shown by running llm templates path , for example:\\n$ llm templates path\\n/Users/simon/Library/Application Support/io.datasette.llm/templates\\nYou can also represent this template as a YAML dictionary with a prompt: key, like this one:\\nprompt:\\'Summarize this: $input \\'\\nOr use YAML multi-line strings for longer inputs. I created this using llm templates edit steampunk :\\nprompt: >\\nSummarize the following text.\\nInsert frequent satirical steampunk-themed illustrative anecdotes.\\nReally go wild with that.\\nText to summarize: $input\\nTheprompt: > causesthefollowingindentedtexttobetreatedasasinglestring,withnewlinescollapsedtospaces.\\nUseprompt: | to preserve newlines.\\nRunningthatwith llm -t steampunk againstGPT-4(viastrip-tagstoremoveHTMLtagsfromtheinputandminify\\nwhitespace):\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio \\'|\\\\\\nstrip-tags -m | llm -t steampunk -m 4\\nOutput:\\nIn a fantastical steampunk world, Simon Willison decided to merge an old MP3 recording with slides\\nfrom the talk using iMovie. After exporting the slides as images and importing them into iMovie, he had\\nto disable the default Ken Burns effect using the “Crop” tool. Then, Simon manually synchronized the\\naudiobyadjustingthedurationofeachimage. Finally,hepublishedthemasterpiecetoYouTube,withthe\\nwhimsical magic of steampunk-infused illustrations leaving his viewers in awe.\\n58 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 64}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n2.9.5 System templates\\nWhen working with models that support system prompts (such as gpt-3.5-turbo andgpt-4) you can set a system\\nprompt using a system: key like so:\\nsystem: Summarize this\\nIf you specify only a system prompt you don’t need to use the $inputvariable - llmwill use the user’s input as the\\nwhole of the regular prompt, which will then be processed using the instructions set in that system prompt.\\nYou can combine system and regular prompts like so:\\nsystem: You speak like an excitable Victorian adventurer\\nprompt:'Summarize this: $input '\\n2.9.6 Additional template variables\\nTemplatesthatworkagainsttheuser’snormalinput(contentthatiseitherpipedtothetoolviastandardinputorpassed\\nas a command-line argument) use just the $inputvariable.\\nYou can use additional named variables. These will then need to be provided using the -p/--param option when\\nexecuting the template.\\nHere’s an example template called recipe, created using llm templates edit recipe :\\nprompt: |\\nSuggest a recipe using ingredients: $ingredients\\nIt should be based on cuisine from this country: $country\\nThis can be executed like so:\\nllm -t recipe -p ingredients 'sausages, milk '-p country Germany\\nMy output started like this:\\nRecipe: German Sausage and Potato Soup\\nIngredients:\\n•4 German sausages\\n•2 cups whole milk\\nThis example combines input piped to the tool with additional parameters. Call this summarize :\\nsystem: Summarize this text in the voice of $voice\\nThen to run it:\\ncurl -s 'https://til.simonwillison.net/macos/imovie-slides-and-audio '|\\\\\\nstrip-tags -m | llm -t summarize -p voice GlaDOS\\nI got this:\\nMy previous test subject seemed to have learned something new about iMovie. They exported keynote\\nslides as individual images [...] Quite impressive for a human.\\n2.9. Prompt templates 59\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 65}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n2.9.7 Specifying default parameters\\nYou can also specify default values for parameters, using a defaults: key.\\nsystem: Summarize this text in the voice of $voice\\ndefaults :\\nvoice: GlaDOS\\nWhen running without -pit will choose the default:\\ncurl -s 'https://til.simonwillison.net/macos/imovie-slides-and-audio '|\\\\\\nstrip-tags -m | llm -t summarize\\nBut you can override the defaults with -p:\\ncurl -s 'https://til.simonwillison.net/macos/imovie-slides-and-audio '|\\\\\\nstrip-tags -m | llm -t summarize -p voice Yoda\\nI got this:\\nText,summarizeinYoda’svoice,Iwill: “Hmm,youngpadawan. Summaryofthistext,youseek. Hmmm.\\n...\\n2.9.8 Setting a default model for a template\\nTemplates executed using llm -t template-name will execute using the default model that the user has configured\\nfor the tool - or gpt-3.5-turbo if they have not configured their own default.\\nYou can specify a new default model for a template using the model:key in the associated YAML. Here’s a template\\ncalledroast:\\nmodel: gpt-4\\nsystem: roast the user at every possible opportunity, be succinct\\nExample:\\nllm -t roast 'How are you today? '\\nI’m doing great but with your boring questions, I must admit, I’ve seen more life in a cemetery.\\n2.10 Logging to SQLite\\nllmdefaults to logging all prompts and responses to a SQLite database.\\nYou can find the location of that database using the llm logs path command:\\nllm logs path\\nOn my Mac that outputs:\\n/Users/simon/Library/Application Support/io.datasette.llm/logs.db\\nThis will differ for other operating systems.\\nTo avoid logging an individual prompt, pass --no-log or-nto the command:\\n60 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 66}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nllm 'Ten names for cheesecakes '-n\\nTo turn logging by default off:\\nllm logs off\\nIf you’ve turned off logging you can still log an individual prompt and response by adding --log:\\nllm 'Five ambitious names for a pet pterodactyl '--log\\nTo turn logging by default back on again:\\nllm logs on\\nTo see the status of the logs database, run this:\\nllm logs status\\nExample output:\\nLogging isONforall prompts\\nFound log database at /Users/simon/Library/Application Support/io.datasette.llm/logs.db\\nNumber of conversations logged: 33\\nNumber of responses logged: 48\\nDatabase file size: 19.96MB\\n2.10.1 Viewing the logs\\nYou can view the logs using the llm logs command:\\nllm logs\\nThis will output the three most recent logged items in Markdown format, showing both the prompt and the response\\nformatted using Markdown.\\nTo get back just the most recent prompt response as plain text, add -r/--response :\\nllm logs -r\\nAdd--jsonto get the log messages in JSON instead:\\nllm logs --json\\nAdd-n 10to see the ten most recent items:\\nllm logs -n 10\\nOr-n 0to see everything that has ever been logged:\\nllm logs -n 0\\nYou can truncate the display of the prompts and responses using the -t/--truncate option. This can help make the\\nJSON output more readable:\\n2.10. Logging to SQLite 61\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 67}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm logs -n 5 -t --json\\nLogs for a conversation\\nTo view the logs for the most recent conversation you have had with a model, use -c:\\nllm logs -c\\nTo see logs for a specific conversation based on its ID, use --cid ID or--conversation ID :\\nllm logs --cid 01h82n0q9crqtnzmf13gkyxawg\\nSearching the logs\\nYou can search the logs for a search term in the promptor theresponse columns.\\nllm logs -q \\'cheesecake \\'\\nThe most relevant terms will be shown at the bottom of the output.\\nFiltering by model\\nYou can filter to logs just for a specific model (or model alias) using -m/--model :\\nllm logs -m chatgpt\\nBrowsing logs using Datasette\\nYou can also use Datasette to browse your logs like this:\\ndatasette \" $(llm logs path )\"\\n2.10.2 SQL schema\\nHere’s the SQL schema used by the logs.db database:\\nCREATE TABLE [conversations] (\\n[id] TEXT PRIMARY KEY ,\\n[name] TEXT,\\n[model] TEXT\\n);\\nCREATE TABLE [responses] (\\n[id] TEXT PRIMARY KEY ,\\n[model] TEXT,\\n[prompt] TEXT,\\n[system] TEXT,\\n[prompt_json] TEXT,\\n[options_json] TEXT,\\n(continues on next page)\\n62 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 68}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n[response] TEXT,\\n[response_json] TEXT,\\n[conversation_id] TEXT REFERENCES [conversations]([id]),\\n[duration_ms] INTEGER,\\n[datetime_utc] TEXT\\n);\\nCREATEVIRTUAL TABLE[responses_fts] USINGFTS5 (\\n[prompt],\\n[response],\\ncontent=[responses]\\n);\\nresponses_fts configures SQLite full-text search against the promptandresponse columns in the responses\\ntable.\\n2.11 Related tools\\nThe following tools are designed to be used with LLM:\\n2.11.1 strip-tags\\nstrip-tags is a command for stripping tags from HTML. This is useful when working with LLMs because HTML tags\\ncan use up a lot of your token budget.\\nHere’showtosummarizethefrontpageoftheNewYorkTimes,bybothstrippingtagsandfilteringtojusttheelements\\nwithclass=\"story-wrapper\" :\\ncurl -s https://www.nytimes.com/ \\\\\\n| strip-tags .story-wrapper \\\\\\n| llm -s \\'summarize the news \\'\\nllm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs describes ways to use strip-tags in\\nmore detail.\\n2.11.2 ttok\\nttok is a command-line tool for counting OpenAI tokens. You can use it to check if input is likely to fit in the token\\nlimit for GPT 3.5 or GPT4:\\ncat my-file.txt | ttok\\n125\\nIt can also truncate input down to a desired number of tokens:\\nttok This is too many tokens -t 3\\nThis istoo\\nThis is useful for truncating a large document down to a size where it can be processed by an LLM.\\n2.11. Related tools 63'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 69}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n2.11.3 Symbex\\nSymbex is a tool for searching for symbols in Python codebases. It’s useful for extracting just the code for a specific\\nproblem and then piping that into LLM for explanation, refactoring or other tasks.\\nHere’s how to use it to find all functions that match test*csv* and use those to guess what the software under test\\ndoes:\\nsymbex 'test*csv* '|\\\\\\nllm --system 'based on these tests guess what this tool does '\\nIt can also be used to export symbols in a format that can be piped to llm embed-multi in order to create embeddings:\\nsymbex '*' '*:*'--nl | \\\\\\nllm embed-multi symbols - \\\\\\n--format nl --database embeddings.db --store\\nFor more examples see Symbex: search Python code for functions and classes, then pipe them into a LLM.\\n2.12 CLI reference\\nThis page lists the --helpoutput for all of the llmcommands.\\n2.12.1 llm –help\\nUsage: llm [OPTIONS] COMMAND [ARGS]...\\nAccess large language models from the command-line\\nDocumentation: https://llm.datasette.io/\\nTo get started, obtain an OpenAI key and set it like this:\\n$ llm keys set openai\\nEnter key: ...\\nThen execute a prompt like this:\\nllm 'Five outrageous names for a pet pelican '\\nOptions:\\n--version Show the version and exit.\\n--help Show this message and exit.\\nCommands:\\nprompt* Execute a prompt\\naliases Manage model aliases\\nchat Hold an ongoing chat with a model.\\ncollections View and manage collections of embeddings\\nembed Embed text and store or return the result\\nembed-models Manage available embedding models\\n(continues on next page)\\n64 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 70}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nembed-multi Store embeddings for multiple strings at once\\ninstall Install packages from PyPI into the same environment as LLM\\nkeys Manage stored API keys for different models\\nlogs Tools for exploring logged prompts and responses\\nmodels Manage available models\\nopenai Commands for working directly with the OpenAI API\\nplugins List installed plugins\\nsimilar Return top N similar IDs from a collection\\ntemplates Manage stored prompt templates\\nuninstall Uninstall Python packages from the LLM environment\\nllm prompt –help\\nUsage: llm prompt [OPTIONS] [PROMPT]\\nExecute a prompt\\nDocumentation: https://llm.datasette.io/en/stable/usage.html\\nOptions:\\n-s, --system TEXT System prompt to use\\n-m, --model TEXT Model to use\\n-o, --option <TEXT TEXT>... key/value options forthe model\\n-t, --template TEXT Template to use\\n-p, --param <TEXT TEXT>... Parameters fortemplate\\n--no-stream Do notstream output\\n-n, --no-log Don 't log to database\\n--log Log prompt andresponse to the database\\n-c, -- continue Continue the most recent conversation.\\n--cid, --conversation TEXT Continue the conversation withthe given ID.\\n--key TEXT API key to use\\n--save TEXT Save prompt withthis template name\\n--help Show this message andexit.\\nllm chat –help\\nUsage: llm chat [OPTIONS]\\nHold an ongoing chat witha model.\\nOptions:\\n-s, --system TEXT System prompt to use\\n-m, --model TEXT Model to use\\n-c, -- continue Continue the most recent conversation.\\n--cid, --conversation TEXT Continue the conversation withthe given ID.\\n-t, --template TEXT Template to use\\n-p, --param <TEXT TEXT>... Parameters fortemplate\\n-o, --option <TEXT TEXT>... key/value options forthe model\\n--no-stream Do notstream output\\n(continues on next page)\\n2.12. CLI reference 65\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 71}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n--key TEXT API key to use\\n--help Show this message andexit.\\nllm keys –help\\nUsage: llm keys [OPTIONS] COMMAND [ARGS]...\\nManage stored API keys fordifferent models\\nOptions:\\n--help Show this message andexit.\\nCommands:\\nlist* List names of all stored keys\\npath Output the path to the keys.json file\\nset Save a key inthe keys.json file\\nllm keys list –help\\nUsage: llm keys list [OPTIONS]\\nList names of all stored keys\\nOptions:\\n--help Show this message andexit.\\nllm keys path –help\\nUsage: llm keys path [OPTIONS]\\nOutput the path to the keys.json file\\nOptions:\\n--help Show this message andexit.\\nllm keys set –help\\nUsage: llm keys set [OPTIONS] NAME\\nSave a key in the keys.json file\\nExample usage:\\n$ llm keys set openai\\nEnter key: ...\\n(continues on next page)\\n66 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 72}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nOptions:\\n--value TEXT Value to set\\n--help Show this message and exit.\\nllm logs –help\\nUsage: llm logs [OPTIONS] COMMAND [ARGS]...\\nTools forexploring logged prompts andresponses\\nOptions:\\n--help Show this message andexit.\\nCommands:\\nlist* Show recent logged prompts andtheir responses\\noff Turn off logging forall prompts\\non Turn on logging forall prompts\\npath Output the path to the logs.db file\\nstatus Show current status of database logging\\nllm logs path –help\\nUsage: llm logs path [OPTIONS]\\nOutput the path to the logs.db file\\nOptions:\\n--help Show this message andexit.\\nllm logs status –help\\nUsage: llm logs status [OPTIONS]\\nShow current status of database logging\\nOptions:\\n--help Show this message andexit.\\n2.12. CLI reference 67'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 73}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm logs on –help\\nUsage: llm logs on [OPTIONS]\\nTurn on logging forall prompts\\nOptions:\\n--help Show this message andexit.\\nllm logs off –help\\nUsage: llm logs off [OPTIONS]\\nTurn off logging forall prompts\\nOptions:\\n--help Show this message andexit.\\nllm logs list –help\\nUsage: llm logs list [OPTIONS]\\nShow recent logged prompts andtheir responses\\nOptions:\\n-n, --count INTEGER Number of entries to show - defaults to 3, use 0\\nforall\\n-p, --path FILE Path to log database\\n-m, --model TEXT Filter by model ormodel alias\\n-q, --query TEXT Search forlogs matching this string\\n-t, --truncate Truncate long strings inoutput\\n-r, --response Just output the last response\\n-c, --current Show logs from the current conversation\\n--cid, --conversation TEXT Show logs forthis conversation ID\\n--json Output logs asJSON\\n--help Show this message andexit.\\nllm models –help\\nUsage: llm models [OPTIONS] COMMAND [ARGS]...\\nManage available models\\nOptions:\\n--help Show this message andexit.\\nCommands:\\n(continues on next page)\\n68 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 74}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nlist* List available models\\ndefault Show orset the default model\\nllm models list –help\\nUsage: llm models list [OPTIONS]\\nList available models\\nOptions:\\n--options Show options foreach model, ifavailable\\n--help Show this message andexit.\\nllm models default –help\\nUsage: llm models default [OPTIONS] [MODEL]\\nShow orset the default model\\nOptions:\\n--help Show this message andexit.\\nllm templates –help\\nUsage: llm templates [OPTIONS] COMMAND [ARGS]...\\nManage stored prompt templates\\nOptions:\\n--help Show this message and exit.\\nCommands:\\nlist* List available prompt templates\\nedit Edit the specified prompt template using the default $EDITOR\\npath Output the path to the templates directory\\nshow Show the specified prompt template\\nllm templates list –help\\nUsage: llm templates list [OPTIONS]\\nList available prompt templates\\nOptions:\\n--help Show this message andexit.\\n2.12. CLI reference 69'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 75}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm templates show –help\\nUsage: llm templates show [OPTIONS] NAME\\nShow the specified prompt template\\nOptions:\\n--help Show this message andexit.\\nllm templates edit –help\\nUsage: llm templates edit [OPTIONS] NAME\\nEdit the specified prompt template using the default $EDITOR\\nOptions:\\n--help Show this message and exit.\\nllm templates path –help\\nUsage: llm templates path [OPTIONS]\\nOutput the path to the templates directory\\nOptions:\\n--help Show this message andexit.\\nllm aliases –help\\nUsage: llm aliases [OPTIONS] COMMAND [ARGS]...\\nManage model aliases\\nOptions:\\n--help Show this message andexit.\\nCommands:\\nlist* List current aliases\\npath Output the path to the aliases.json file\\nremove Remove an alias\\nset Set an alias fora model\\n70 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 76}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm aliases list –help\\nUsage: llm aliases list [OPTIONS]\\nList current aliases\\nOptions:\\n--json Output asJSON\\n--help Show this message andexit.\\nllm aliases set –help\\nUsage: llm aliases set [OPTIONS] ALIAS MODEL_ID\\nSet an alias for a model\\nExample usage:\\n$ llm aliases set turbo gpt-3.5-turbo\\nOptions:\\n--help Show this message and exit.\\nllm aliases remove –help\\nUsage: llm aliases remove [OPTIONS] ALIAS\\nRemove an alias\\nExample usage:\\n$ llm aliases remove turbo\\nOptions:\\n--help Show this message and exit.\\nllm aliases path –help\\nUsage: llm aliases path [OPTIONS]\\nOutput the path to the aliases.json file\\nOptions:\\n--help Show this message andexit.\\n2.12. CLI reference 71'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 77}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nllm plugins –help\\nUsage: llm plugins [OPTIONS]\\nList installed plugins\\nOptions:\\n--all Include built- indefault plugins\\n--help Show this message andexit.\\nllm install –help\\nUsage: llm install [OPTIONS] [PACKAGES]...\\nInstall packages from PyPI into the same environment asLLM\\nOptions:\\n-U, --upgrade Upgrade packages to latest version\\n-e, --editable TEXT Install a project ineditable mode from this path\\n--force-reinstall Reinstall all packages even ifthey are already up-to-\\ndate\\n--no-cache-dir Disable the cache\\n--help Show this message andexit.\\nllm uninstall –help\\nUsage: llm uninstall [OPTIONS] PACKAGES...\\nUninstall Python packages from the LLM environment\\nOptions:\\n-y, --yes Don 't ask for confirmation\\n--help Show this message andexit.\\nllm embed –help\\nUsage: llm embed [OPTIONS] [COLLECTION] [ID]\\nEmbed text andstore or return the result\\nOptions:\\n-i, --input PATH File to embed\\n-m, --model TEXT Embedding model to use\\n--store Store the text itself inthe database\\n-d, --database FILE\\n-c, --content TEXT Content to embed\\n--binary Treat input asbinary data\\n--metadata TEXT JSON object metadata to store\\n(continues on next page)\\n72 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 78}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n-f, --format [json|blob|base64|hex]\\nOutput format\\n--help Show this message andexit.\\nllm embed-multi –help\\nUsage: llm embed-multi [OPTIONS] COLLECTION [INPUT_PATH]\\nStore embeddings formultiple strings at once\\nInput can be CSV, TSV ora JSON list of objects.\\nThe first column istreated asan ID - all other columns are assumed to be\\ntext that should be concatenated together inorder to calculate the\\nembeddings.\\nInput data can come from one of three sources:\\n1. A CSV, JSON, TSV orJSON-nl file (including on standard input)\\n2. A SQL query against a SQLite database\\n3. A directory of files\\nOptions:\\n--format [json|csv|tsv|nl] Format of input file - defaults to auto-detect\\n--files <DIRECTORY TEXT>... Embed files inthis directory - specify directory\\nandglob pattern\\n--encoding TEXT Encoding to use when reading --files\\n--binary Treat --files asbinary data\\n--sql TEXT Read input using this SQL query\\n--attach <TEXT FILE>... Additional databases to attach - specify alias\\nandfile path\\n--batch-size INTEGER Batch size to use when running embeddings\\n--prefix TEXT Prefix to add to the IDs\\n-m, --model TEXT Embedding model to use\\n--store Store the text itself inthe database\\n-d, --database FILE\\n--help Show this message andexit.\\nllm similar –help\\nUsage: llm similar [OPTIONS] COLLECTION [ID]\\nReturn top N similar IDs from acollection\\nExample usage:\\nllm similar my-collection -c \"I like cats\"\\nOr to find content similar to a specific stored ID:\\n(continues on next page)\\n2.12. CLI reference 73'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 79}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\nllm similar my-collection 1234\\nOptions:\\n-i, --input PATH File to embed forcomparison\\n-c, --content TEXT Content to embed forcomparison\\n--binary Treat input asbinary data\\n-n, --number INTEGER Number of results to return\\n-d, --database FILE\\n--help Show this message andexit.\\nllm embed-models –help\\nUsage: llm embed-models [OPTIONS] COMMAND [ARGS]...\\nManage available embedding models\\nOptions:\\n--help Show this message andexit.\\nCommands:\\nlist* List available embedding models\\ndefault Show orset the default embedding model\\nllm embed-models list –help\\nUsage: llm embed-models list [OPTIONS]\\nList available embedding models\\nOptions:\\n--help Show this message andexit.\\nllm embed-models default –help\\nUsage: llm embed-models default [OPTIONS] [MODEL]\\nShow orset the default embedding model\\nOptions:\\n--remove-default Reset to specifying no default model\\n--help Show this message andexit.\\n74 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 80}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nllm collections –help\\nUsage: llm collections [OPTIONS] COMMAND [ARGS]...\\nView andmanage collections of embeddings\\nOptions:\\n--help Show this message andexit.\\nCommands:\\nlist* View a list of collections\\ndelete Delete the specified collection\\npath Output the path to the embeddings database\\nllm collections path –help\\nUsage: llm collections path [OPTIONS]\\nOutput the path to the embeddings database\\nOptions:\\n--help Show this message andexit.\\nllm collections list –help\\nUsage: llm collections list [OPTIONS]\\nView a list of collections\\nOptions:\\n-d, --database FILE Path to embeddings database\\n--json Output asJSON\\n--help Show this message andexit.\\nllm collections delete –help\\nUsage: llm collections delete [OPTIONS] COLLECTION\\nDelete the specified collection\\nExample usage:\\nllm collections delete my-collection\\nOptions:\\n-d, --database FILE Path to embeddings database\\n--help Show this message andexit.\\n2.12. CLI reference 75'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 81}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\nllm openai –help\\nUsage: llm openai [OPTIONS] COMMAND [ARGS]...\\nCommands forworking directly withthe OpenAI API\\nOptions:\\n--help Show this message andexit.\\nCommands:\\nmodels List models available to you from the OpenAI API\\nllm openai models –help\\nUsage: llm openai models [OPTIONS]\\nList models available to you from the OpenAI API\\nOptions:\\n--json Output asJSON\\n--key TEXT OpenAI API key\\n--help Show this message andexit.\\n2.13 Contributing\\nTo contribute to this tool, first checkout the code. Then create a new virtual environment:\\ncd llm\\npython -m venv venv\\nsource venv/bin/activate\\nOr if you are using pipenv:\\npipenv shell\\nNow install the dependencies and test dependencies:\\npip install -e '.[test] '\\nTo run the tests:\\npytest\\n76 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 82}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n2.13.1 Debugging tricks\\nThedefaultOpenAIpluginhasadebuggingmechanismforshowingtheexactrequestsandresponsesthatweresentto\\nthe OpenAI API.\\nSet theLLM_OPENAI_SHOW_RESPONSES environment variable like this:\\nLLM_OPENAI_SHOW_RESPONSES=1 llm -m chatgpt 'three word slogan for an an otter-run bakery '\\nThis will output details of the API requests and responses to the console.\\nUse--no-stream to see a more readable version of the body that avoids streaming the response:\\nLLM_OPENAI_SHOW_RESPONSES=1 llm -m chatgpt --no-stream \\\\\\n'three word slogan for an an otter-run bakery '\\n2.13.2 Documentation\\nDocumentation for this project uses MyST - it is written in Markdown and rendered using Sphinx.\\nTo build the documentation locally, run the following:\\ncd docs\\npip install -r requirements.txt\\nmake livehtml\\nThis will start a live preview server, using sphinx-autobuild.\\nThe CLI--helpexamples in the documentation are managed using Cog. Update those files like this:\\njust cog\\nYou’ll need Just installed to run this command.\\n2.13.3 Release process\\nTo release a new version:\\n1. Update docs/changelog.md with the new changes.\\n2. Update the version number in setup.py\\n3. Create a GitHub release for the new version.\\n4. Wait for the package to push to PyPI and then...\\n5. Run the regenerate.yaml workflow to update the Homebrew tap to the latest version.\\n2.13. Contributing 77\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 83}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.14 Changelog\\n2.14.1 0.16 (2024-09-12)\\n•OpenAImodelsnowusetheinternal self.get_key() mechanism,whichmeanstheycanbeusedfromPython\\ncode in a way that will pick up keys that have been configured using llm keys set or theOPENAI_API_KEY\\nenvironment variable. #552. This code now works correctly:\\nimport llm\\nprint(llm.get_model(\"gpt-4o-mini\").prompt(\"hi\"))\\n•New documented API methods: llm.get_default_model() ,llm.set_default_model(alias) ,llm.\\nget_default_embedding_model(alias) ,llm.set_default_embedding_model() . #553\\n•Support for OpenAI’s new o1 family of preview models, llm -m o1-preview \"prompt\" andllm -m\\no1-mini \"prompt\" . These models are currently only available to tier 5 OpenAI API users, though this may\\nchange in the future. #570\\n2.14.2 0.15 (2024-07-18)\\n•Support for OpenAI’s new GPT-4o mini model: llm -m gpt-4o-mini \\'rave about pelicans in\\nFrench \\'#536\\n•gpt-4o-mini isnowthedefaultmodelifyoudonot specifyyourowndefault ,replacingGPT-3.5Turbo. GPT-4o\\nmini is both cheaper and better than GPT-3.5 Turbo.\\n•Fixedabugwhere llm logs -q \\'flourish \\'-m haiku couldnotcombineboththe -qsearchqueryandthe\\n-mmodel specifier. #515\\n2.14.3 0.14 (2024-05-13)\\n•Support for OpenAI’s new GPT-4o model: llm -m gpt-4o \\'say hi in Spanish \\'#490\\n•Thegpt-4-turbo alias is now a model ID, which indicates the latest version of OpenAI’s GPT-4 Turbo\\ntext and image model. Your existing logs.db database may contain records under the previous model ID of\\ngpt-4-turbo-preview . #493\\n•Newllm logs -r/--response option for outputting just the last captured response, without wrapping it in\\nMarkdown and accompanying it with the prompt. #431\\n•Nine new pluginssince version 0.13:\\n– llm-claude-3 supporting Anthropic’s Claude 3 family of models.\\n– llm-command-r supporting Cohere’s Command R and Command R Plus API models.\\n– llm-reka supports the Reka family of models via their API.\\n– llm-perplexity by Alexandru Geana supporting the Perplexity Labs API models, including\\nllama-3-sonar-large-32k-online whichcansearchforthingsonlineand llama-3-70b-instruct .\\n– llm-groq by Moritz Angermann providing access to fast models hosted by Groq.\\n– llm-fireworks supporting models hosted by Fireworks AI.\\n– llm-together adds support for the Together AI extensive family of hosted openly licensed models.\\n– llm-embed-onnx provides seven embedding models that can be executed using the ONNX model frame-\\nwork.\\n78 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 84}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n– llm-cmd accepts a prompt for a shell command, runs that prompt and populates the result in your shell so\\nyou can review it, edit it and then hit <enter> to execute or ctrl+cto cancel, see this post for details.\\n2.14.4 0.13.1 (2024-01-26)\\n•Fix forNo module named 'readline 'error on Windows. #407\\n2.14.5 0.13 (2024-01-26)\\nSee also LLM 0.13: The annotated release notes.\\n•Added support for new OpenAI embedding models: 3-small and3-large and three variants of those with\\ndifferent dimension sizes, 3-small-512 ,3-large-256 and3-large-1024 . SeeOpenAI embedding models\\nfor details. #394\\n•Thedefault gpt-4-turbo modelaliasnowpointsto gpt-4-turbo-preview ,whichusesthemostrecentOpe-\\nnAI GPT-4 turbo model (currently gpt-4-0125-preview ). #396\\n•New OpenAI model aliases gpt-4-1106-preview andgpt-4-0125-preview .\\n•OpenAI models now support a -o json_object 1 option which will cause their output to be returned as a\\nvalid JSON object. #373\\n•Newpluginssince the last release include llm-mistral, llm-gemini, llm-ollama and llm-bedrock-meta.\\n•Thekeys.json file for storing API keys is now created with 600file permissions. #351\\n•Documented apatternforinstallingpluginsthatdependonPyTorchusingtheHomebrewversionofLLM,despite\\nHomebrewusingPython3.12whenPyTorchhavenotyetreleasedastablepackageforthatPythonversion. #397\\n•Underlying OpenAI Python library has been upgraded to >1.0. It is possible this could cause compatibility\\nissues with LLM plugins that also depend on that library. #325\\n•Arrow keys now work inside the llm chat command. #376\\n•LLM_OPENAI_SHOW_RESPONSES=1 environment variable now outputs much more detailed information about\\nthe HTTP request and response made to OpenAI (and OpenAI-compatible) APIs. #404\\n•Dropped support for Python 3.7.\\n2.14.6 0.12 (2023-11-06)\\n•SupportforthenewGPT-4TurbomodelfromOpenAI.Tryitusing llm chat -m gpt-4-turbo orllm chat\\n-m 4t. #323\\n•New-o seed 1 optionforOpenAImodelswhichsetsaseedthatcanattempttoevaluatethepromptdetermin-\\nistically. #324\\n2.14. Changelog 79\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 85}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.14.7 0.11.2 (2023-11-06)\\n•Pin to version of OpenAI Python library prior to 1.0 to avoid breaking. #327\\n2.14.8 0.11.1 (2023-10-31)\\n•Fixed a bug where llm embed -c \"text\" did not correctly pick up the configured default embedding model .\\n#317\\n•New plugins: llm-python, llm-bedrock-anthropic and llm-embed-jina (described in Execute Jina embeddings\\nwith a CLI using llm-embed-jina).\\n•llm-gpt4all now uses the new GGUF model format. simonw/llm-gpt4all#16\\n2.14.9 0.11 (2023-09-18)\\nLLMnowsupportsthenewOpenAI gpt-3.5-turbo-instruct model,andOpenAIcompletion(asopposedtochat\\ncompletion) models in general. #284\\nllm -m gpt-3.5-turbo-instruct \\'Reasons to tame a wild beaver: \\'\\nOpenAI completion models like this support a -o logprobs 3 option, which accepts a number between 1 and 5 and\\nwill include the log probabilities (for each produced token, what were the top 3 options considered by the model) in\\nthe logged response.\\nllm -m gpt-3.5-turbo-instruct \\'Say hello succinctly \\'-o logprobs 3\\nYou can then view the logprobs that were recorded in the SQLite logs database like this:\\nsqlite-utils \" $(llm logs path )\"\\\\\\n\\'select * from responses order by id desc limit 1 \\'|\\\\\\njq\\'.[0].response_json \\'-r | jq\\nTruncated output looks like this:\\n[\\n{\\n\"text\": \"Hi\",\\n\"top_logprobs\": [\\n{\\n\"Hi\": -0.13706253,\\n\"Hello\": -2.3714375,\\n\"Hey\": -3.3714373\\n}\\n]\\n},\\n{\\n\"text\": \" there\",\\n\"top_logprobs\": [\\n{\\n\" there\": -0.96057636,\\n\"!\\\\\"\": -0.5855763,\\n\".\\\\\"\": -3.2574513\\n(continues on next page)\\n80 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 86}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n}\\n]\\n}\\n]\\nAlso in this release:\\n•Thellm.user_dir() function, used by plugins, now ensures the directory exists before returning it. #275\\n•NewLLM_OPENAI_SHOW_RESPONSES=1 environment variable for displaying the full HTTP response returned\\nby OpenAI compatible APIs. #286\\n•Thellm embed-multi command now has a --batch-size X option for setting the batch size to use when\\nprocessing embeddings - useful if you have limited memory available. #273\\n•Thecollection.embed_multi() method also now accepts an optional batch_size=int argument.\\n•Fixed two bugs with llm embed-multi --files relating to handling of directories. Thanks, ealvar3z. #274,\\n#280\\n2.14.10 0.10 (2023-09-12)\\nThe two major features in this release are the llm chat command and support for embedding binary data .\\nSeeBuildanimagesearchenginewithllm-clip,chatwithmodelswithllmchatformorebackgroundonthesefeatures.\\nllm chat\\nThe newllm chat command starts an ongoing chat conversation with a model in your terminal. It works with all\\nmodels supported by LLM and its plugins, including locally installed models such as Llama 2. #231\\nThis offers a big performance boost for local models, since they don’t need to be freshly loaded into memory for each\\nprompt.\\nHere’s an example chat with Llama 2 13B, provided by the llm-mlc plugin.\\nllm chat -m mlc-chat-Llama-2-13b-chat-hf-q4f16_1\\nThis starts a chat session:\\nType \\'exit \\'or\\'quit \\'to exit\\nType \\'!multi \\'to enter multiple lines, then \\'!end \\'to finish\\n> Who are you?\\nHello! I \\'m just an AI, here to assist you with any questions you may have.\\nMy name is LLaMA, and I \\'m a large language model trained to provide helpful\\nand informative responses to a wide range of questions and topics. I \\'m here\\nto help you find the information you need, while ensuring a safe and\\nrespectful conversation. Please feel free to ask me anything, and I \\'ll do my\\nbest to provide a helpful and accurate response.\\n> Tell me a joke about otters\\nSure, here \\'s a joke about otters:\\nWhy did the otter go to the party?\\nBecause he heard it was a \"whale\" of a time!\\n(continues on next page)\\n2.14. Changelog 81'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 87}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n(continued from previous page)\\n(Get it? Whale, like a big sea mammal, but also a \"wild\" or \"fun\" time.\\nOtters are known for their playful and social nature, so it \\'s a lighthearted\\nand silly joke.)\\nI hope that brought a smile to your face! Do you have any other questions or\\ntopics you \\'d like to discuss?\\n> exit\\nChat sessions are logged to SQLite - usellm logs to view them. They can accept system prompts, templates and\\nmodel options - consult the chat documentation for details.\\nBinary embedding support\\nLLM’sembeddingsfeature hasbeenexpandedtoprovidesupportforembeddingbinarydata,inadditiontotext. #254\\nThis enables models like CLIP, supported by the new llm-clipplugin.\\nCLIP is a multi-modal embedding model which can embed images and text into the same vector space. This means\\nyoucanuseittocreateanembeddingindexofphotos,andthensearchfortheembeddingvectorfor“ahappydog”and\\nget back images that are semantically closest to that string.\\nTo create embeddings for every JPEG in a directory stored in a photoscollection, run:\\nllm install llm-clip\\nllm embed-multi photos --files photos/ \\'*.jpg \\'--binary -m clip\\nNow you can search for photos of raccoons using:\\nllm similar photos -c \\'raccoon \\'\\nThis spits out a list of images, ranked by how similar they are to the string “raccoon”:\\n{\"id\": \"IMG_4801.jpeg\", \"score\": 0.28125139257127457, \"content\": null, \"metadata\": null}\\n{\"id\": \"IMG_4656.jpeg\", \"score\": 0.26626441704164294, \"content\": null, \"metadata\": null}\\n{\"id\": \"IMG_2944.jpeg\", \"score\": 0.2647445926996852, \"content\": null, \"metadata\": null}\\n...\\nAlso in this release\\n•TheLLM_LOAD_PLUGINS environment variable can be used to control which plugins are loaded when llm\\nstarts running. #256\\n•Thellm plugins --all option includes builtin plugins in the list of plugins. #259\\n•Thellm embed-db family of commands has been renamed to llm collections . #229\\n•llm embed-multi --files now has an --encoding option and defaults to falling back to latin-1 if a file\\ncannot be processed as utf-8. #225\\n82 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 88}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.14.11 0.10a1 (2023-09-11)\\n•Support for embedding binary data. #254\\n•llm chat now works for models with API keys. #247\\n•llm chat -o for passing options to a model. #244\\n•llm chat --no-stream option. #248\\n•LLM_LOAD_PLUGINS environment variable. #256\\n•llm plugins --all option for including builtin plugins. #259\\n•llm embed-db has been renamed to llm collections . #229\\n•Fixed bug where llm embed -c option was treated as a filepath, not a string. Thanks, mhalle. #263\\n2.14.12 0.10a0 (2023-09-04)\\n•Newllm chatcommand for starting an interactive terminal chat with a model. #231\\n•llm embed-multi --files now has an --encoding option and defaults to falling back to latin-1 if a file\\ncannot be processed as utf-8. #225\\n2.14.13 0.9 (2023-09-03)\\nThebignewfeatureinthisreleaseissupportfor embeddings . SeeLLMnowprovidestoolsforworkingwithembed-\\ndings for additional details.\\nEmbedding models take a piece of text - a word, sentence, paragraph or even a whole article, and convert that into an\\narray of floating point numbers. #185\\nThis embedding vector can be thought of as representing a position in many-dimensional-space, where the distance\\nbetweentwovectorsrepresentshowsemanticallysimilartheyaretoeachotherwithinthecontentofalanguagemodel.\\nEmbeddingscanbeusedtofind relateddocuments ,andalsotoimplement semanticsearch -whereausercansearch\\nforaphraseandgetbackresultsthataresemanticallysimilartothatphraseeveniftheydonotshareanyexactkeywords.\\nLLM now provides both CLI and Python APIs for working with embeddings. Embedding models are defined by\\nplugins, so you can install additional models using the plugins mechanism .\\nThe first two embedding models supported by LLM are:\\n•OpenAI’sada-002embeddingmodel,availableviaaninexpensiveAPIifyousetanOpenAIkeyusing llm keys\\nset openai .\\n•The sentence-transformers family of models, available via the new llm-sentence-transformers plugin.\\nSeeEmbedding with the CLI for detailed instructions on working with embeddings using LLM.\\nThe new commands for working with embeddings are:\\n•llm embed -calculateembeddingsforcontentandreturnthemtotheconsoleorstoretheminaSQLitedatabase.\\n•llm embed-multi - run bulk embeddings for multiple strings, using input from a CSV, TSV or JSON file, data\\nfrom a SQLite database or data found by scanning the filesystem. #215\\n•llm similar - run similarity searches against your stored embeddings - starting with a search phrase or finding\\ncontent related to a previously stored vector. #190\\n•llm embed-models - list available embedding models.\\n2.14. Changelog 83'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 89}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n•llm embed-db - commands for inspecting and working with the default embeddings SQLite database.\\nThere’s also a new llm.Collection class for creating and searching collections of embedding from Python code, and a\\nllm.get_embedding_model() interface for embedding strings directly. #191\\n2.14.14 0.8.1 (2023-08-31)\\n•Fixedbugwherefirstpromptwouldshowanerrorifthe io.datasette.llm directoryhadnotyetbeencreated.\\n#193\\n•Updateddocumentationtorecommendadifferent llm-gpt4all modelsincetheonewewereusingisnolonger\\navailable. #195\\n2.14.15 0.8 (2023-08-20)\\n•The output format for llm logs has changed. Previously it was JSON - it’s now a much more readable Mark-\\ndown format suitable for pasting into other documents. #160\\n–The newllm logs --json option can be used to get the old JSON format.\\n–Passllm logs --conversation ID or--cid ID to see the full logs for a specific conversation.\\n•You can now combine piped input and a prompt in a single command: cat script.py | llm 'explain\\nthis code '. This works even for models that do not support system prompts . #153\\n•Additional OpenAI-compatible models can now be configured with custom HTTP headers. This enables plat-\\nforms such as openrouter.ai to be used with LLM, which can provide Claude access even without an Anthropic\\nAPI key.\\n•Keys set in keys.json are now used in preference to environment variables. #158\\n•The documentation now includes a plugin directory listing all available plugins for LLM. #173\\n•Newrelated tools section in the documentation describing ttok,strip-tags andsymbex. #111\\n•Thellm models ,llm aliases andllm templates commands now default to running the same command\\nasllm models list andllm aliases list andllm templates list . #167\\n•Newllm keys (akallm keys list ) command for listing the names of all configured keys. #174\\n•Two new Python API functions, llm.set_alias(alias, model_id) andllm.remove_alias(alias) can\\nbe used to configure aliases from within Python code. #154\\n•LLM is now compatible with both Pydantic 1 and Pydantic 2. This means you can install llmas a Python\\ndependency in a project that depends on Pydantic 1 without running into dependency conflicts. Thanks, Chris\\nMungall. #147\\n•llm.get_model(model_id) isnowdocumentedasraising llm.UnknownModelError iftherequestedmodel\\ndoes not exist. #155\\n84 Chapter 2. Contents\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 90}, page_content=\"LLM documentation, Release 0.16-1-gd654c95\\n2.14.16 0.7.1 (2023-08-19)\\n•Fixedabugwheresomeuserswouldseean AlterError: No such column: log.id errorwhenattempt-\\ning to use this tool, after upgrading to the latest sqlite-utils 3.35 release. #162\\n2.14.17 0.7 (2023-08-12)\\nThe newModel aliases commands can be used to configure additional aliases for models, for example:\\nllm aliases set turbo gpt-3.5-turbo-16k\\nNow you can run the 16,000 token gpt-3.5-turbo-16k model like this:\\nllm -m turbo 'An epic Greek-style saga about a cheesecake that builds a SQL database␣\\n˓→from scratch '\\nUsellm aliases list to see a list of aliases and llm aliases remove turbo to remove one again. #151\\nNotable new plugins\\n•llm-mlccanrunlocalmodelsreleasedbytheMLCproject,includingmodelsthatcantakeadvantageoftheGPU\\non Apple Silicon M1/M2 devices.\\n•llm-llama-cpp uses llama.cpp to run models published in the GGML format. See Run Llama 2 on your own\\nMac using LLM and Homebrew for more details.\\nAlso in this release\\n•OpenAI models now have min and max validation on their floating point options. Thanks, Pavel Král. #115\\n•Fix for bug where llm templates list raised an error if a template had an empty prompt. Thanks, Sherwin\\nDaganato. #132\\n•Fixed bug in llm install --editable option which prevented installation of .[test]. #136\\n•llm install --no-cache-dir and--force-reinstall options. #146\\n2.14.18 0.6.1 (2023-07-24)\\n•LLM can now be installed directly from Homebrew core: brew install llm . #124\\n•Python API documentation now covers System prompts .\\n•Fixed incorrect example in the Prompt templates documentation. Thanks, Jorge Cabello. #125\\n2.14. Changelog 85\"),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 91}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.14.19 0.6 (2023-07-18)\\n•ModelshostedonReplicatecannowbeaccessedusingthellm-replicateplugin,includingthenewLlama2model\\nfrom Meta AI. More details here: Accessing Llama 2 from the command-line with the llm-replicate plugin.\\n•ModelprovidersthatexposeanAPIthatiscompatiblewiththeOpenAPIAPIformat,includingself-hostedmodel\\nserverssuchasLocalAI,cannowbeaccessedusing additionalconfiguration forthedefaultOpenAIplugin. #106\\n•OpenAI models that are not yet supported by LLM can also be configured using the new\\nextra-openai-models.yaml configuration file. #107\\n•Thellmlogscommand nowacceptsa -m model_id optiontofilterlogstoaspecificmodel. Aliasescanbeused\\nhere in addition to model IDs. #108\\n•Logs now have a SQLite full-text search index against their prompts and responses, and the llm logs -q\\nSEARCHoption can be used to return logs that match a search term. #109\\n2.14.20 0.5 (2023-07-12)\\nLLMnowsupports additionallanguagemodels ,thankstoanew pluginsmechanism forinstallingadditionalmodels.\\nPlugins are available for 19 models in addition to the default OpenAI ones:\\n•llm-gpt4alladdssupportfor17modelsthatcandownloadandrunonyourowndevice,includingVicuna,Falcon\\nand wizardLM.\\n•llm-mpt30b adds support for the MPT-30B model, a 19GB download.\\n•llm-palm adds support for Google’s PaLM 2 via the Google API.\\nA comprehensive tutorial, writing a plugin to support a new model describes how to add new models by building\\nplugins in detail.\\nNew features\\n•Python API documentation for using LLM models, including models from plugins, directly from Python. #75\\n•Messages are now logged to the database by default - no need to run the llm init-db command any more,\\nwhich has been removed. Instead, you can toggle this behavior off using llm logs off or turn it on again\\nusingllm logs on . Thellm logs status commandshowsthecurrentstatusofthelogdatabase. Iflogging\\nis turned off, passing --logto thellm prompt command will cause that prompt to be logged anyway. #98\\n•Newdatabaseschemaforloggedmessages,with conversations andresponses tables. Ifyouhavepreviously\\nused the old logstable it will continue to exist but will no longer be written to. #91\\n•New-o/--option name value syntaxforsettingoptionsformodels,suchastemperature. Availableoptions\\ndiffer for different models. #63\\n•llm models list --options command for viewing all available model options. #82\\n•llm \"prompt\" --save template option for saving a prompt directly to a template. #55\\n•Prompt templates can now specify default values for parameters. Thanks, Chris Mungall. #57\\n•llm openai models command to list all available OpenAI models from their API. #70\\n•llm models default MODEL_ID tosetadifferentmodelasthedefaulttobeusedwhen llmisrunwithoutthe\\n-m/--model option. #31\\n86 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 92}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nSmaller improvements\\n•llm -sis now a shortcut for llm --system . #69\\n•llm -m 4-32k alias forgpt-4-32k .\\n•llm install -e directory command for installing a plugin from a local directory.\\n•TheLLM_USER_PATH environment variable now controls the location of the directory in which LLM stores its\\ndata. This replaces the old LLM_KEYS_PATH andLLM_LOG_PATH andLLM_TEMPLATES_PATH variables. #76\\n•Documentation covering Utility functions for plugins .\\n•Documentation site now uses Plausible for analytics. #79\\n2.14.21 0.4.1 (2023-06-17)\\n•LLM can now be installed using Homebrew: brew install simonw/llm/llm . #50\\n•llmis now styled LLM in the documentation. #45\\n•Examples in documentation now include a copy button. #43\\n•llm templates command no longer has its display disrupted by newlines. #42\\n•llm templates command now includes system prompt, if set. #44\\n2.14.22 0.4 (2023-06-17)\\nThis release includes some backwards-incompatible changes:\\n•The-4option for GPT-4 is now -m 4.\\n•The--codeoption has been removed.\\n•The-soption has been removed as streaming is now the default. Use --no-stream to opt out of streaming.\\nPrompt templates\\nPrompt templates is a new feature that allows prompts to be saved as templates and re-used with different variables.\\nTemplates can be created using the llm templates edit command:\\nllm templates edit summarize\\nTemplates are YAML - the following template defines summarization using a system prompt:\\nsystem: Summarize this text\\nThe template can then be executed like this:\\ncat myfile.txt | llm -t summarize\\nTemplatescanincludebothsystemprompts,regularpromptsandindicatethemodeltheyshoulduse. Theycanreference\\nvariables such as $inputfor content piped to the tool, or other variables that are passed using the new -p/--param\\noption.\\nThis example adds a voiceparameter:\\n2.14. Changelog 87'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 93}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nsystem: Summarize this text in the voice of $voice\\nThen to run it (via strip-tags to remove HTML tags from the input):\\ncurl -s \\'https://til.simonwillison.net/macos/imovie-slides-and-audio \\'|\\\\\\nstrip-tags -m | llm -t summarize -p voice GlaDOS\\nExample output:\\nMy previous test subject seemed to have learned something new about iMovie. They exported keynote\\nslides as individual images [...] Quite impressive for a human.\\nThePrompt templates documentation provides more detailed examples.\\nContinue previous chat\\nYoucannowuse llmtocontinueapreviousconversationwiththeOpenAIchatmodels( gpt-3.5-turbo andgpt-4).\\nThis will include your previous prompts and responses in the prompt sent to the API, allowing the model to continue\\nwithin the same context.\\nUse the new -c/--continue option to continue from the previous message thread:\\nllm \"Pretend to be a witty gerbil, say hi briefly\"\\nGreetings,dearhuman! Iamaclevergerbil,readytoentertainyouwithmyquickwitandendlessenergy.\\nllm \"What do you think of snacks?\" -c\\nOh, how I adore snacks, dear human! Crunchy carrot sticks, sweet apple slices, and chewy yogurt drops\\nare some of my favorite treats. I could nibble on them all day long!\\nThe-coption will continue from the most recent logged message.\\nTo continue a different chat, pass an integer ID to the --chatoption. This should be the ID of a previously logged\\nmessage. You can find these IDs using the llm logs command.\\nThanks Amjith Ramanujam for contributing to this feature. #6\\nNew mechanism for storing API keys\\nAPIkeysforlanguagemodelssuchasthosebyOpenAIcannowbesavedusingthenew llm keys familyofcommands.\\nTo set the default key to be used for the OpenAI APIs, run this:\\nllm keys set openai\\nThen paste in your API key.\\nKeyscanalsobepassedusingthenew --keycommandlineoption-thiscanbeafullkeyorthealiasofakeythathas\\nbeen previously stored.\\nSeeAPI key management for more. #13\\n88 Chapter 2. Contents'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 94}, page_content='LLM documentation, Release 0.16-1-gd654c95\\nNew location for the logs.db database\\nThelogs.db database that stores a history of executed prompts no longer lives at ~/.llm/log.db - it can now be\\nfound in a location that better fits the host operating system, which can be seen using:\\nllm logs path\\nOn macOS this is ~/Library/Application Support/io.datasette.llm/logs.db .\\nTo open that database using Datasette, run this:\\ndatasette \" $(llm logs path )\"\\nYou can upgrade your existing installation by copying your database to the new location like this:\\ncp ~/.llm/log.db \" $(llm logs path )\"\\nrm -rf ~/.llm # To tidy up the now obsolete directory\\nThe database schema has changed, and will be updated automatically the first time you run the command.\\nThat schema is included in the documentation. #35\\nOther changes\\n•Newllm logs --truncate option(shortcut -t)whichtruncatesthedisplayedpromptstomakethelogoutput\\neasier to read. #16\\n•Documentation now spans multiple pages and lives at https://llm.datasette.io/ #21\\n•Defaultllm chatgpt command has been renamed to llm prompt . #17\\n•Removed --codeoption in favour of new prompt templates mechanism. #24\\n•Responses are now streamed by default, if the model supports streaming. The -s/--stream option has been\\nremoved. A new --no-stream option can be used to opt-out of streaming. #25\\n•The-4/--gpt4 option has been removed in favour of -m 4or-m gpt4, using a new mechanism that allows\\nmodels to have additional short names.\\n•The newgpt-3.5-turbo-16k model with a 16,000 token context length can now also be accessed using -m\\nchatgpt-16k or-m 3.5-16k . Thanks, Benjamin Kirkbride. #37\\n•Improved display of error messages from OpenAI. #15\\n2.14.23 0.3 (2023-05-17)\\n•llm logs command for browsing logs of previously executed completions. #3\\n•llm \"Python code to output factorial 10\" --code option which sets a system prompt designed to\\nencourage code to be output without any additional explanatory text. #5\\n•Tool can now accept a prompt piped directly to standard input. #11\\n2.14. Changelog 89'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 95}, page_content='LLM documentation, Release 0.16-1-gd654c95\\n2.14.24 0.2 (2023-04-01)\\n•If a SQLite database exists in ~/.llm/log.db all prompts and responses are logged to that file. The llm\\ninit-db command can be used to create this file. #2\\n2.14.25 0.1 (2023-04-01)\\n•Initial prototype release. #1\\n90 Chapter 2. Contents')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"llm-datasette-io-en-latest.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 0}, page_content='LLM documentation\\nRelease 0.16-1-gd654c95\\nSimon Willison\\nSep 12, 2024'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 2}, page_content='CONTENTS\\n1 Quick start 3\\n2 Contents 5\\n2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.1 Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.2 Upgrading to the latest version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.3 A note about Homebrew and PyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.4 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.5 API key management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1.6 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2.1 Executing a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 2}, page_content='2.2.1 Executing a prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2.2 Completion prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.3 Continuing a conversation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.4 Using with a shell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.2.5 System prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.2.6 Starting an interactive chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.2.7 Listing available models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3 OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.3.1 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 2}, page_content='2.3.1 Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.3.2 OpenAI language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.3.3 OpenAI embedding models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.3.4 Adding more OpenAI models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.4 Other models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.4.1 Installing and using a local model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.4.2 OpenAI-compatible models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.5 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.5.1 Embedding with the CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20'),\n",
       " Document(metadata={'source': 'llm-datasette-io-en-latest.pdf', 'page': 2}, page_content='2.5.1 Embedding with the CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.5.2 Using embeddings from Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.5.3 Writing plugins to add new embedding models . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.5.4 Embedding storage format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.6 Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.6.1 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.6.2 Plugin directory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.6.3 Plugin hooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.6.4 Writing a plugin to support a new model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)[:5]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma vector store initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create embeddings\n",
    "try:    \n",
    "    embeddings = OllamaEmbeddings()\n",
    "except:\n",
    "    print(\"!\")\n",
    "try:\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    print(\"Chroma vector store initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS imported successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    print(\"FAISS imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1 Embedding with the CLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.5.2 Using embeddings from Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.5.3 Writing plugins to add new embedding models . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.5.4 Embedding storage format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n2.6 Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.6.1 Installing plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n2.6.2 Plugin directory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2.6.3 Plugin hooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n2.6.4 Writing a plugin to support a new model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what ia llm\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    answer the following question based only on the provided context.\n",
    "    Think step by step before prooviding a detailed answer.\n",
    "    I will tip you $1000 if the user finds the answer helpful.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\n",
    "                                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000271DE129CC0>, search_kwargs={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"An attention function can be described as mapping a query\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_pretice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
